{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mahmmoudsalah/ddi-drugsafe-output?scriptVersionId=210899932\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"6ed71ca9","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:08:22.652951Z","iopub.status.busy":"2024-12-02T23:08:22.652657Z","iopub.status.idle":"2024-12-02T23:08:26.544505Z","shell.execute_reply":"2024-12-02T23:08:26.543683Z"},"papermill":{"duration":3.897507,"end_time":"2024-12-02T23:08:26.54661","exception":false,"start_time":"2024-12-02T23:08:22.649103","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'GNN_DDI'...\r\n","remote: Enumerating objects: 163, done.\u001b[K\r\n","remote: Counting objects: 100% (161/161), done.\u001b[K\r\n","remote: Compressing objects: 100% (134/134), done.\u001b[K\r\n","remote: Total 163 (delta 76), reused 80 (delta 24), pack-reused 2 (from 1)\u001b[K\r\n","Receiving objects: 100% (163/163), 43.64 MiB | 27.07 MiB/s, done.\r\n","Resolving deltas: 100% (76/76), done.\r\n"]}],"source":["!git clone --branch main https://github.com/OmarKhalifaa/GNN_DDI.git\n"," "]},{"cell_type":"code","execution_count":2,"id":"ce72c992","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:08:26.554787Z","iopub.status.busy":"2024-12-02T23:08:26.554192Z","iopub.status.idle":"2024-12-02T23:08:26.56061Z","shell.execute_reply":"2024-12-02T23:08:26.559841Z"},"papermill":{"duration":0.012003,"end_time":"2024-12-02T23:08:26.562111","exception":false,"start_time":"2024-12-02T23:08:26.550108","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/GNN_DDI\n"]}],"source":["%cd /kaggle/working/GNN_DDI\n"]},{"cell_type":"code","execution_count":3,"id":"999b8f31","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:08:26.570481Z","iopub.status.busy":"2024-12-02T23:08:26.56988Z","iopub.status.idle":"2024-12-02T23:08:31.091423Z","shell.execute_reply":"2024-12-02T23:08:31.090509Z"},"papermill":{"duration":4.527511,"end_time":"2024-12-02T23:08:31.093364","exception":false,"start_time":"2024-12-02T23:08:26.565853","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Tables :  [('event_number',), ('event',), ('drug',), ('extraction',)]\r\n","drug :  [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\r\n","event :  [('index',), ('id1',), ('name1',), ('id2',), ('name2',), ('interaction',)]\r\n","event row COUNT:  [(37264,)]\r\n","0    P14780|Q00653|P01375|P01579|P33673\r\n","1                                Q02641\r\n","Name: target, dtype: object\r\n","   index  ...          name\r\n","0      0  ...   Glucosamine\r\n","1      1  ...  Azelnidipine\r\n","\r\n","[2 rows x 7 columns]\r\n","/kaggle/working/GNN_DDI/prepare.py:53: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\r\n","You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\r\n","A typical example is when you are setting values in a column of a DataFrame, like:\r\n","\r\n","df[\"col\"][row_indexer] = value\r\n","\r\n","Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\r\n","\r\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\r\n","\r\n","  df_feature[each_feature].iloc[i] = 1\r\n","Traceback (most recent call last):\r\n","  File \"/kaggle/working/GNN_DDI/prepare.py\", line 76, in <module>\r\n","    mat = feature_vector(df_drug, feature)\r\n","  File \"/kaggle/working/GNN_DDI/prepare.py\", line 58, in feature_vector\r\n","    pca.fit(sim_matrix)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\", line 435, in fit\r\n","    self._fit(X)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\", line 485, in _fit\r\n","    X = self._validate_data(\r\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 565, in _validate_data\r\n","    X = check_array(X, input_name=\"X\", **check_params)\r\n","  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 737, in check_array\r\n","    raise TypeError(\r\n","TypeError: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\r\n"]}],"source":["!python prepare.py"]},{"cell_type":"code","execution_count":4,"id":"1da9ec20","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:08:31.102708Z","iopub.status.busy":"2024-12-02T23:08:31.102379Z","iopub.status.idle":"2024-12-02T23:08:32.299311Z","shell.execute_reply":"2024-12-02T23:08:32.298425Z"},"papermill":{"duration":1.20407,"end_time":"2024-12-02T23:08:32.301292","exception":false,"start_time":"2024-12-02T23:08:31.097222","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["From https://github.com/OmarKhalifaa/GNN_DDI\r\n"," * branch            main       -> FETCH_HEAD\r\n","Already up to date.\r\n"]}],"source":["!git pull origin main"]},{"cell_type":"code","execution_count":5,"id":"1639f297","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:08:32.30956Z","iopub.status.busy":"2024-12-02T23:08:32.309276Z","iopub.status.idle":"2024-12-02T23:19:42.531198Z","shell.execute_reply":"2024-12-02T23:19:42.529966Z"},"papermill":{"duration":670.228418,"end_time":"2024-12-02T23:19:42.533241","exception":false,"start_time":"2024-12-02T23:08:32.304823","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(input='/kaggle/working/GNN_DDI/DDI/data5', features='/kaggle/working/GNN_DDI/DDI/data5/featuers_m1.txt', walk_file=None, epoch=1, batch_size=64, eval_type='all', schema=None, dimensions=32, edge_dim=10, att_dim=20, walk_length=10, num_walks=20, window_size=5, negative_samples=5, neighbor_samples=10, patience=5, num_workers=16, file=None)\r\n","We are loading data from: /kaggle/working/GNN_DDI/DDI/data5/train.txt\r\n","Total training nodes: 570\r\n","We are loading data from: /kaggle/working/GNN_DDI/DDI/data5/valid.txt\r\n","We are loading data from: /kaggle/working/GNN_DDI/DDI/data5/test.txt\r\n","Generating random walks for layer 0\r\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n","  self.pid = os.fork()\r\n","7940it [00:00, 108295.01it/s]\r\n","Generating random walks for layer 1\r\n","8980it [00:00, 78605.51it/s]\r\n","Generating random walks for layer 2\r\n","9640it [00:00, 43749.14it/s]\r\n","Generating random walks for layer 3\r\n","8440it [00:00, 120385.80it/s]\r\n","Generating random walks for layer 4\r\n","6100it [00:00, 372501.34it/s]\r\n","Generating random walks for layer 5\r\n","2220it [00:00, 443482.32it/s]\r\n","Generating random walks for layer 6\r\n","1680it [00:00, 430053.75it/s]\r\n","Generating random walks for layer 7\r\n","2000it [00:00, 406365.74it/s]\r\n","Generating random walks for layer 8\r\n","5400it [00:00, 445106.45it/s]\r\n","Generating random walks for layer 9\r\n","1600it [00:00, 430322.95it/s]\r\n","Generating random walks for layer 10\r\n","1040it [00:00, 330285.16it/s]\r\n","Generating random walks for layer 11\r\n","1620it [00:00, 424302.02it/s]\r\n","Generating random walks for layer 12\r\n","2900it [00:00, 542600.78it/s]\r\n","Generating random walks for layer 13\r\n","740it [00:00, 275990.13it/s]\r\n","Generating random walks for layer 14\r\n","980it [00:00, 343508.10it/s]\r\n","Generating random walks for layer 15\r\n","1020it [00:00, 319291.74it/s]\r\n","Generating random walks for layer 16\r\n","680it [00:00, 275222.11it/s]\r\n","Generating random walks for layer 17\r\n","1060it [00:00, 341000.33it/s]\r\n","Generating random walks for layer 18\r\n","1340it [00:00, 355067.75it/s]\r\n","Generating random walks for layer 19\r\n","880it [00:00, 339027.05it/s]\r\n","Generating random walks for layer 20\r\n","460it [00:00, 179878.78it/s]\r\n","Generating random walks for layer 21\r\n","620it [00:00, 259864.94it/s]\r\n","Generating random walks for layer 22\r\n","840it [00:00, 300205.81it/s]\r\n","Generating random walks for layer 23\r\n","440it [00:00, 198461.53it/s]\r\n","Generating random walks for layer 24\r\n","300it [00:00, 138410.65it/s]\r\n","Generating random walks for layer 25\r\n","800it [00:00, 315539.14it/s]\r\n","Generating random walks for layer 26\r\n","880it [00:00, 291179.20it/s]\r\n","Generating random walks for layer 27\r\n","980it [00:00, 341595.44it/s]\r\n","Generating random walks for layer 28\r\n","680it [00:00, 284161.28it/s]\r\n","Generating random walks for layer 29\r\n","380it [00:00, 165181.42it/s]\r\n","Generating random walks for layer 30\r\n","480it [00:00, 208628.59it/s]\r\n","Generating random walks for layer 31\r\n","660it [00:00, 275173.03it/s]\r\n","Generating random walks for layer 32\r\n","560it [00:00, 213353.64it/s]\r\n","Generating random walks for layer 33\r\n","420it [00:00, 170615.76it/s]\r\n","Generating random walks for layer 34\r\n","560it [00:00, 235022.04it/s]\r\n","Generating random walks for layer 35\r\n","520it [00:00, 191470.29it/s]\r\n","Generating random walks for layer 36\r\n","420it [00:00, 187904.82it/s]\r\n","Generating random walks for layer 37\r\n","420it [00:00, 190053.69it/s]\r\n","Generating random walks for layer 38\r\n","640it [00:00, 272302.15it/s]\r\n","Generating random walks for layer 39\r\n","620it [00:00, 202955.47it/s]\r\n","Generating random walks for layer 40\r\n","580it [00:00, 224750.21it/s]\r\n","Generating random walks for layer 41\r\n","360it [00:00, 160359.97it/s]\r\n","Generating random walks for layer 42\r\n","380it [00:00, 172009.01it/s]\r\n","Generating random walks for layer 43\r\n","540it [00:00, 212848.81it/s]\r\n","Generating random walks for layer 44\r\n","260it [00:00, 122599.11it/s]\r\n","Generating random walks for layer 45\r\n","400it [00:00, 185465.58it/s]\r\n","Generating random walks for layer 46\r\n","300it [00:00, 128659.63it/s]\r\n","Generating random walks for layer 47\r\n","260it [00:00, 118508.92it/s]\r\n","Generating random walks for layer 48\r\n","280it [00:00, 128687.83it/s]\r\n","Generating random walks for layer 49\r\n","160it [00:00, 452521.00it/s]\r\n","Generating random walks for layer 50\r\n","200it [00:00, 410200.88it/s]\r\n","Generating random walks for layer 51\r\n","220it [00:00, 564371.18it/s]\r\n","Generating random walks for layer 52\r\n","120it [00:00, 327466.81it/s]\r\n","Generating random walks for layer 53\r\n","120it [00:00, 281654.44it/s]\r\n","Generating random walks for layer 54\r\n","120it [00:00, 372275.50it/s]\r\n","Generating random walks for layer 55\r\n","140it [00:00, 419131.02it/s]\r\n","Generating random walks for layer 56\r\n","120it [00:00, 189287.88it/s]\r\n","Generating random walks for layer 57\r\n","120it [00:00, 382750.17it/s]\r\n","Generating random walks for layer 58\r\n","100it [00:00, 322638.77it/s]\r\n","Generating random walks for layer 59\r\n","80it [00:00, 247817.08it/s]\r\n","Generating random walks for layer 60\r\n","80it [00:00, 234646.38it/s]\r\n","Generating random walks for layer 61\r\n","80it [00:00, 127583.39it/s]\r\n","Generating random walks for layer 62\r\n","80it [00:00, 236132.53it/s]\r\n","Generating random walks for layer 63\r\n","120it [00:00, 378718.19it/s]\r\n","Generating random walks for layer 64\r\n","80it [00:00, 270600.26it/s]\r\n","Finish generating the walks\r\n","Saving walks for layer 0\r\n","100%|███████████████████████████████████| 7940/7940 [00:00<00:00, 507061.33it/s]\r\n","Saving walks for layer 1\r\n","100%|███████████████████████████████████| 8980/8980 [00:00<00:00, 513326.93it/s]\r\n","Saving walks for layer 2\r\n","100%|███████████████████████████████████| 9640/9640 [00:00<00:00, 530541.40it/s]\r\n","Saving walks for layer 3\r\n","100%|███████████████████████████████████| 8440/8440 [00:00<00:00, 536435.66it/s]\r\n","Saving walks for layer 4\r\n","100%|███████████████████████████████████| 6100/6100 [00:00<00:00, 519940.95it/s]\r\n","Saving walks for layer 5\r\n","100%|███████████████████████████████████| 2220/2220 [00:00<00:00, 553127.89it/s]\r\n","Saving walks for layer 6\r\n","100%|███████████████████████████████████| 1680/1680 [00:00<00:00, 547763.58it/s]\r\n","Saving walks for layer 7\r\n","100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 538663.58it/s]\r\n","Saving walks for layer 8\r\n","100%|███████████████████████████████████| 5400/5400 [00:00<00:00, 565905.64it/s]\r\n","Saving walks for layer 9\r\n","100%|███████████████████████████████████| 1600/1600 [00:00<00:00, 561862.56it/s]\r\n","Saving walks for layer 10\r\n","100%|███████████████████████████████████| 1040/1040 [00:00<00:00, 575927.67it/s]\r\n","Saving walks for layer 11\r\n","100%|███████████████████████████████████| 1620/1620 [00:00<00:00, 547832.98it/s]\r\n","Saving walks for layer 12\r\n","100%|███████████████████████████████████| 2900/2900 [00:00<00:00, 580152.70it/s]\r\n","Saving walks for layer 13\r\n","100%|█████████████████████████████████████| 740/740 [00:00<00:00, 550023.92it/s]\r\n","Saving walks for layer 14\r\n","100%|█████████████████████████████████████| 980/980 [00:00<00:00, 580075.91it/s]\r\n","Saving walks for layer 15\r\n","100%|███████████████████████████████████| 1020/1020 [00:00<00:00, 572946.31it/s]\r\n","Saving walks for layer 16\r\n","100%|█████████████████████████████████████| 680/680 [00:00<00:00, 367589.47it/s]\r\n","Saving walks for layer 17\r\n","100%|███████████████████████████████████| 1060/1060 [00:00<00:00, 455855.86it/s]\r\n","Saving walks for layer 18\r\n","100%|███████████████████████████████████| 1340/1340 [00:00<00:00, 577573.46it/s]\r\n","Saving walks for layer 19\r\n","100%|█████████████████████████████████████| 880/880 [00:00<00:00, 570830.11it/s]\r\n","Saving walks for layer 20\r\n","100%|█████████████████████████████████████| 460/460 [00:00<00:00, 545176.56it/s]\r\n","Saving walks for layer 21\r\n","100%|█████████████████████████████████████| 620/620 [00:00<00:00, 566427.46it/s]\r\n","Saving walks for layer 22\r\n","100%|█████████████████████████████████████| 840/840 [00:00<00:00, 575782.87it/s]\r\n","Saving walks for layer 23\r\n","100%|█████████████████████████████████████| 440/440 [00:00<00:00, 504646.91it/s]\r\n","Saving walks for layer 24\r\n","100%|█████████████████████████████████████| 300/300 [00:00<00:00, 524944.18it/s]\r\n","Saving walks for layer 25\r\n","100%|█████████████████████████████████████| 800/800 [00:00<00:00, 575745.23it/s]\r\n","Saving walks for layer 26\r\n","100%|█████████████████████████████████████| 880/880 [00:00<00:00, 574383.37it/s]\r\n","Saving walks for layer 27\r\n","100%|█████████████████████████████████████| 980/980 [00:00<00:00, 579748.65it/s]\r\n","Saving walks for layer 28\r\n","100%|█████████████████████████████████████| 680/680 [00:00<00:00, 565898.16it/s]\r\n","Saving walks for layer 29\r\n","100%|█████████████████████████████████████| 380/380 [00:00<00:00, 544714.81it/s]\r\n","Saving walks for layer 30\r\n","100%|█████████████████████████████████████| 480/480 [00:00<00:00, 528139.01it/s]\r\n","Saving walks for layer 31\r\n","100%|█████████████████████████████████████| 660/660 [00:00<00:00, 555202.70it/s]\r\n","Saving walks for layer 32\r\n","100%|█████████████████████████████████████| 560/560 [00:00<00:00, 555536.95it/s]\r\n","Saving walks for layer 33\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 566615.53it/s]\r\n","Saving walks for layer 34\r\n","100%|█████████████████████████████████████| 560/560 [00:00<00:00, 558177.34it/s]\r\n","Saving walks for layer 35\r\n","100%|█████████████████████████████████████| 520/520 [00:00<00:00, 567535.28it/s]\r\n","Saving walks for layer 36\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 547593.31it/s]\r\n","Saving walks for layer 37\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 542199.96it/s]\r\n","Saving walks for layer 38\r\n","100%|█████████████████████████████████████| 640/640 [00:00<00:00, 548499.09it/s]\r\n","Saving walks for layer 39\r\n","100%|█████████████████████████████████████| 620/620 [00:00<00:00, 555655.66it/s]\r\n","Saving walks for layer 40\r\n","100%|█████████████████████████████████████| 580/580 [00:00<00:00, 557829.93it/s]\r\n","Saving walks for layer 41\r\n","100%|█████████████████████████████████████| 360/360 [00:00<00:00, 524106.02it/s]\r\n","Saving walks for layer 42\r\n","100%|█████████████████████████████████████| 380/380 [00:00<00:00, 543600.11it/s]\r\n","Saving walks for layer 43\r\n","100%|█████████████████████████████████████| 540/540 [00:00<00:00, 561597.86it/s]\r\n","Saving walks for layer 44\r\n","100%|█████████████████████████████████████| 260/260 [00:00<00:00, 524540.18it/s]\r\n","Saving walks for layer 45\r\n","100%|█████████████████████████████████████| 400/400 [00:00<00:00, 541025.99it/s]\r\n","Saving walks for layer 46\r\n","100%|█████████████████████████████████████| 300/300 [00:00<00:00, 544950.71it/s]\r\n","Saving walks for layer 47\r\n","100%|█████████████████████████████████████| 260/260 [00:00<00:00, 527585.41it/s]\r\n","Saving walks for layer 48\r\n","100%|█████████████████████████████████████| 280/280 [00:00<00:00, 500172.54it/s]\r\n","Saving walks for layer 49\r\n","100%|█████████████████████████████████████| 160/160 [00:00<00:00, 457456.47it/s]\r\n","Saving walks for layer 50\r\n","100%|█████████████████████████████████████| 200/200 [00:00<00:00, 519097.03it/s]\r\n","Saving walks for layer 51\r\n","100%|█████████████████████████████████████| 220/220 [00:00<00:00, 534615.81it/s]\r\n","Saving walks for layer 52\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 438428.99it/s]\r\n","Saving walks for layer 53\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 513064.71it/s]\r\n","Saving walks for layer 54\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 494416.97it/s]\r\n","Saving walks for layer 55\r\n","100%|█████████████████████████████████████| 140/140 [00:00<00:00, 545220.58it/s]\r\n","Saving walks for layer 56\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 594234.33it/s]\r\n","Saving walks for layer 57\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 522654.70it/s]\r\n","Saving walks for layer 58\r\n","100%|█████████████████████████████████████| 100/100 [00:00<00:00, 581734.26it/s]\r\n","Saving walks for layer 59\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 478665.22it/s]\r\n","Saving walks for layer 60\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 561110.90it/s]\r\n","Saving walks for layer 61\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 492000.47it/s]\r\n","Saving walks for layer 62\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 391991.03it/s]\r\n","Saving walks for layer 63\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 516752.03it/s]\r\n","Saving walks for layer 64\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 477983.36it/s]\r\n","Counting vocab for layer 0\r\n","100%|███████████████████████████████████| 7940/7940 [00:00<00:00, 579773.57it/s]\r\n","Counting vocab for layer 1\r\n","100%|███████████████████████████████████| 8980/8980 [00:00<00:00, 662425.47it/s]\r\n","Counting vocab for layer 2\r\n","100%|███████████████████████████████████| 9640/9640 [00:00<00:00, 649798.96it/s]\r\n","Counting vocab for layer 3\r\n","100%|███████████████████████████████████| 8440/8440 [00:00<00:00, 576564.80it/s]\r\n","Counting vocab for layer 4\r\n","100%|███████████████████████████████████| 6100/6100 [00:00<00:00, 607639.16it/s]\r\n","Counting vocab for layer 5\r\n","100%|███████████████████████████████████| 2220/2220 [00:00<00:00, 627280.71it/s]\r\n","Counting vocab for layer 6\r\n","100%|███████████████████████████████████| 1680/1680 [00:00<00:00, 558398.50it/s]\r\n","Counting vocab for layer 7\r\n","100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 564205.54it/s]\r\n","Counting vocab for layer 8\r\n","100%|███████████████████████████████████| 5400/5400 [00:00<00:00, 659174.67it/s]\r\n","Counting vocab for layer 9\r\n","100%|███████████████████████████████████| 1600/1600 [00:00<00:00, 638523.92it/s]\r\n","Counting vocab for layer 10\r\n","100%|███████████████████████████████████| 1040/1040 [00:00<00:00, 591628.40it/s]\r\n","Counting vocab for layer 11\r\n","100%|███████████████████████████████████| 1620/1620 [00:00<00:00, 678324.10it/s]\r\n","Counting vocab for layer 12\r\n","100%|███████████████████████████████████| 2900/2900 [00:00<00:00, 658054.62it/s]\r\n","Counting vocab for layer 13\r\n","100%|█████████████████████████████████████| 740/740 [00:00<00:00, 634331.69it/s]\r\n","Counting vocab for layer 14\r\n","100%|█████████████████████████████████████| 980/980 [00:00<00:00, 542916.12it/s]\r\n","Counting vocab for layer 15\r\n","100%|███████████████████████████████████| 1020/1020 [00:00<00:00, 637679.25it/s]\r\n","Counting vocab for layer 16\r\n","100%|█████████████████████████████████████| 680/680 [00:00<00:00, 642370.88it/s]\r\n","Counting vocab for layer 17\r\n","100%|███████████████████████████████████| 1060/1060 [00:00<00:00, 673426.57it/s]\r\n","Counting vocab for layer 18\r\n","100%|███████████████████████████████████| 1340/1340 [00:00<00:00, 651561.25it/s]\r\n","Counting vocab for layer 19\r\n","100%|█████████████████████████████████████| 880/880 [00:00<00:00, 631477.76it/s]\r\n","Counting vocab for layer 20\r\n","100%|█████████████████████████████████████| 460/460 [00:00<00:00, 661426.07it/s]\r\n","Counting vocab for layer 21\r\n","100%|█████████████████████████████████████| 620/620 [00:00<00:00, 715003.71it/s]\r\n","Counting vocab for layer 22\r\n","100%|█████████████████████████████████████| 840/840 [00:00<00:00, 661264.14it/s]\r\n","Counting vocab for layer 23\r\n","100%|█████████████████████████████████████| 440/440 [00:00<00:00, 659576.04it/s]\r\n","Counting vocab for layer 24\r\n","100%|█████████████████████████████████████| 300/300 [00:00<00:00, 691748.87it/s]\r\n","Counting vocab for layer 25\r\n","100%|█████████████████████████████████████| 800/800 [00:00<00:00, 677867.31it/s]\r\n","Counting vocab for layer 26\r\n","100%|█████████████████████████████████████| 880/880 [00:00<00:00, 676624.66it/s]\r\n","Counting vocab for layer 27\r\n","100%|█████████████████████████████████████| 980/980 [00:00<00:00, 662329.67it/s]\r\n","Counting vocab for layer 28\r\n","100%|█████████████████████████████████████| 680/680 [00:00<00:00, 649095.75it/s]\r\n","Counting vocab for layer 29\r\n","100%|█████████████████████████████████████| 380/380 [00:00<00:00, 635754.10it/s]\r\n","Counting vocab for layer 30\r\n","100%|█████████████████████████████████████| 480/480 [00:00<00:00, 652386.88it/s]\r\n","Counting vocab for layer 31\r\n","100%|█████████████████████████████████████| 660/660 [00:00<00:00, 679322.86it/s]\r\n","Counting vocab for layer 32\r\n","100%|█████████████████████████████████████| 560/560 [00:00<00:00, 688598.72it/s]\r\n","Counting vocab for layer 33\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 620721.52it/s]\r\n","Counting vocab for layer 34\r\n","100%|█████████████████████████████████████| 560/560 [00:00<00:00, 697804.59it/s]\r\n","Counting vocab for layer 35\r\n","100%|█████████████████████████████████████| 520/520 [00:00<00:00, 681148.68it/s]\r\n","Counting vocab for layer 36\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 675463.07it/s]\r\n","Counting vocab for layer 37\r\n","100%|█████████████████████████████████████| 420/420 [00:00<00:00, 606614.21it/s]\r\n","Counting vocab for layer 38\r\n","100%|█████████████████████████████████████| 640/640 [00:00<00:00, 718318.05it/s]\r\n","Counting vocab for layer 39\r\n","100%|█████████████████████████████████████| 620/620 [00:00<00:00, 674045.74it/s]\r\n","Counting vocab for layer 40\r\n","100%|█████████████████████████████████████| 580/580 [00:00<00:00, 705743.06it/s]\r\n","Counting vocab for layer 41\r\n","100%|█████████████████████████████████████| 360/360 [00:00<00:00, 674686.97it/s]\r\n","Counting vocab for layer 42\r\n","100%|█████████████████████████████████████| 380/380 [00:00<00:00, 656980.84it/s]\r\n","Counting vocab for layer 43\r\n","100%|█████████████████████████████████████| 540/540 [00:00<00:00, 689894.66it/s]\r\n","Counting vocab for layer 44\r\n","100%|█████████████████████████████████████| 260/260 [00:00<00:00, 662929.51it/s]\r\n","Counting vocab for layer 45\r\n","100%|█████████████████████████████████████| 400/400 [00:00<00:00, 650531.83it/s]\r\n","Counting vocab for layer 46\r\n","100%|█████████████████████████████████████| 300/300 [00:00<00:00, 694421.19it/s]\r\n","Counting vocab for layer 47\r\n","100%|█████████████████████████████████████| 260/260 [00:00<00:00, 704014.87it/s]\r\n","Counting vocab for layer 48\r\n","100%|█████████████████████████████████████| 280/280 [00:00<00:00, 590747.04it/s]\r\n","Counting vocab for layer 49\r\n","100%|█████████████████████████████████████| 160/160 [00:00<00:00, 637916.96it/s]\r\n","Counting vocab for layer 50\r\n","100%|█████████████████████████████████████| 200/200 [00:00<00:00, 582947.05it/s]\r\n","Counting vocab for layer 51\r\n","100%|█████████████████████████████████████| 220/220 [00:00<00:00, 637696.53it/s]\r\n","Counting vocab for layer 52\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 590747.04it/s]\r\n","Counting vocab for layer 53\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 662258.53it/s]\r\n","Counting vocab for layer 54\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 638726.50it/s]\r\n","Counting vocab for layer 55\r\n","100%|█████████████████████████████████████| 140/140 [00:00<00:00, 615516.31it/s]\r\n","Counting vocab for layer 56\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 616809.41it/s]\r\n","Counting vocab for layer 57\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 645277.54it/s]\r\n","Counting vocab for layer 58\r\n","100%|█████████████████████████████████████| 100/100 [00:00<00:00, 556273.74it/s]\r\n","Counting vocab for layer 59\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 622531.21it/s]\r\n","Counting vocab for layer 60\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 578524.69it/s]\r\n","Counting vocab for layer 61\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 557382.59it/s]\r\n","Counting vocab for layer 62\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 672433.51it/s]\r\n","Counting vocab for layer 63\r\n","100%|█████████████████████████████████████| 120/120 [00:00<00:00, 661388.28it/s]\r\n","Counting vocab for layer 64\r\n","100%|███████████████████████████████████████| 80/80 [00:00<00:00, 486296.12it/s]\r\n","Generating training pairs for layer 0\r\n","100%|████████████████████████████████████| 7940/7940 [00:00<00:00, 60722.62it/s]\r\n","Generating training pairs for layer 1\r\n","100%|████████████████████████████████████| 8980/8980 [00:00<00:00, 60091.94it/s]\r\n","Generating training pairs for layer 2\r\n","100%|████████████████████████████████████| 9640/9640 [00:00<00:00, 59255.21it/s]\r\n","Generating training pairs for layer 3\r\n","100%|████████████████████████████████████| 8440/8440 [00:00<00:00, 61463.43it/s]\r\n","Generating training pairs for layer 4\r\n","100%|████████████████████████████████████| 6100/6100 [00:00<00:00, 64352.79it/s]\r\n","Generating training pairs for layer 5\r\n","100%|████████████████████████████████████| 2220/2220 [00:00<00:00, 58928.15it/s]\r\n","Generating training pairs for layer 6\r\n","100%|████████████████████████████████████| 1680/1680 [00:00<00:00, 36251.36it/s]\r\n","Generating training pairs for layer 7\r\n","100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 63187.21it/s]\r\n","Generating training pairs for layer 8\r\n","100%|████████████████████████████████████| 5400/5400 [00:00<00:00, 64965.86it/s]\r\n","Generating training pairs for layer 9\r\n","100%|████████████████████████████████████| 1600/1600 [00:00<00:00, 65923.56it/s]\r\n","Generating training pairs for layer 10\r\n","100%|████████████████████████████████████| 1040/1040 [00:00<00:00, 62434.00it/s]\r\n","Generating training pairs for layer 11\r\n","100%|████████████████████████████████████| 1620/1620 [00:00<00:00, 65255.92it/s]\r\n","Generating training pairs for layer 12\r\n","100%|████████████████████████████████████| 2900/2900 [00:00<00:00, 63776.64it/s]\r\n","Generating training pairs for layer 13\r\n","100%|██████████████████████████████████████| 740/740 [00:00<00:00, 63231.57it/s]\r\n","Generating training pairs for layer 14\r\n","100%|██████████████████████████████████████| 980/980 [00:00<00:00, 63617.93it/s]\r\n","Generating training pairs for layer 15\r\n","100%|████████████████████████████████████| 1020/1020 [00:00<00:00, 61345.73it/s]\r\n","Generating training pairs for layer 16\r\n","100%|██████████████████████████████████████| 680/680 [00:00<00:00, 64043.80it/s]\r\n","Generating training pairs for layer 17\r\n","100%|████████████████████████████████████| 1060/1060 [00:00<00:00, 65178.59it/s]\r\n","Generating training pairs for layer 18\r\n","100%|████████████████████████████████████| 1340/1340 [00:00<00:00, 62777.76it/s]\r\n","Generating training pairs for layer 19\r\n","100%|██████████████████████████████████████| 880/880 [00:00<00:00, 65367.71it/s]\r\n","Generating training pairs for layer 20\r\n","100%|██████████████████████████████████████| 460/460 [00:00<00:00, 63612.92it/s]\r\n","Generating training pairs for layer 21\r\n","100%|██████████████████████████████████████| 620/620 [00:00<00:00, 66218.54it/s]\r\n","Generating training pairs for layer 22\r\n","100%|██████████████████████████████████████| 840/840 [00:00<00:00, 60310.44it/s]\r\n","Generating training pairs for layer 23\r\n","100%|██████████████████████████████████████| 440/440 [00:00<00:00, 64175.46it/s]\r\n","Generating training pairs for layer 24\r\n","100%|██████████████████████████████████████| 300/300 [00:00<00:00, 64923.96it/s]\r\n","Generating training pairs for layer 25\r\n","100%|██████████████████████████████████████| 800/800 [00:00<00:00, 64295.30it/s]\r\n","Generating training pairs for layer 26\r\n","100%|██████████████████████████████████████| 880/880 [00:00<00:00, 64684.94it/s]\r\n","Generating training pairs for layer 27\r\n","100%|██████████████████████████████████████| 980/980 [00:00<00:00, 60394.92it/s]\r\n","Generating training pairs for layer 28\r\n","100%|██████████████████████████████████████| 680/680 [00:00<00:00, 64583.28it/s]\r\n","Generating training pairs for layer 29\r\n","100%|██████████████████████████████████████| 380/380 [00:00<00:00, 64311.65it/s]\r\n","Generating training pairs for layer 30\r\n","100%|██████████████████████████████████████| 480/480 [00:00<00:00, 61684.72it/s]\r\n","Generating training pairs for layer 31\r\n","100%|██████████████████████████████████████| 660/660 [00:00<00:00, 64287.99it/s]\r\n","Generating training pairs for layer 32\r\n","100%|██████████████████████████████████████| 560/560 [00:00<00:00, 50732.43it/s]\r\n","Generating training pairs for layer 33\r\n","100%|██████████████████████████████████████| 420/420 [00:00<00:00, 58632.31it/s]\r\n","Generating training pairs for layer 34\r\n","100%|██████████████████████████████████████| 560/560 [00:00<00:00, 60626.97it/s]\r\n","Generating training pairs for layer 35\r\n","100%|██████████████████████████████████████| 520/520 [00:00<00:00, 66178.30it/s]\r\n","Generating training pairs for layer 36\r\n","100%|██████████████████████████████████████| 420/420 [00:00<00:00, 65387.61it/s]\r\n","Generating training pairs for layer 37\r\n","100%|██████████████████████████████████████| 420/420 [00:00<00:00, 66730.09it/s]\r\n","Generating training pairs for layer 38\r\n","100%|██████████████████████████████████████| 640/640 [00:00<00:00, 62887.54it/s]\r\n","Generating training pairs for layer 39\r\n","100%|██████████████████████████████████████| 620/620 [00:00<00:00, 65259.70it/s]\r\n","Generating training pairs for layer 40\r\n","100%|██████████████████████████████████████| 580/580 [00:00<00:00, 65342.37it/s]\r\n","Generating training pairs for layer 41\r\n","100%|██████████████████████████████████████| 360/360 [00:00<00:00, 62173.66it/s]\r\n","Generating training pairs for layer 42\r\n","100%|██████████████████████████████████████| 380/380 [00:00<00:00, 63114.70it/s]\r\n","Generating training pairs for layer 43\r\n","100%|██████████████████████████████████████| 540/540 [00:00<00:00, 64249.52it/s]\r\n","Generating training pairs for layer 44\r\n","100%|██████████████████████████████████████| 260/260 [00:00<00:00, 57056.40it/s]\r\n","Generating training pairs for layer 45\r\n","100%|██████████████████████████████████████| 400/400 [00:00<00:00, 60491.13it/s]\r\n","Generating training pairs for layer 46\r\n","100%|██████████████████████████████████████| 300/300 [00:00<00:00, 65913.63it/s]\r\n","Generating training pairs for layer 47\r\n","100%|██████████████████████████████████████| 260/260 [00:00<00:00, 66825.11it/s]\r\n","Generating training pairs for layer 48\r\n","100%|██████████████████████████████████████| 280/280 [00:00<00:00, 64966.82it/s]\r\n","Generating training pairs for layer 49\r\n","100%|██████████████████████████████████████| 160/160 [00:00<00:00, 65173.22it/s]\r\n","Generating training pairs for layer 50\r\n","100%|██████████████████████████████████████| 200/200 [00:00<00:00, 62610.90it/s]\r\n","Generating training pairs for layer 51\r\n","100%|██████████████████████████████████████| 220/220 [00:00<00:00, 62895.98it/s]\r\n","Generating training pairs for layer 52\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 63461.92it/s]\r\n","Generating training pairs for layer 53\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 65238.69it/s]\r\n","Generating training pairs for layer 54\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 65681.39it/s]\r\n","Generating training pairs for layer 55\r\n","100%|██████████████████████████████████████| 140/140 [00:00<00:00, 65918.56it/s]\r\n","Generating training pairs for layer 56\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 64346.26it/s]\r\n","Generating training pairs for layer 57\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 64329.82it/s]\r\n","Generating training pairs for layer 58\r\n","100%|██████████████████████████████████████| 100/100 [00:00<00:00, 64113.48it/s]\r\n","Generating training pairs for layer 59\r\n","100%|████████████████████████████████████████| 80/80 [00:00<00:00, 63382.00it/s]\r\n","Generating training pairs for layer 60\r\n","100%|████████████████████████████████████████| 80/80 [00:00<00:00, 61197.21it/s]\r\n","Generating training pairs for layer 61\r\n","100%|████████████████████████████████████████| 80/80 [00:00<00:00, 60176.53it/s]\r\n","Generating training pairs for layer 62\r\n","100%|████████████████████████████████████████| 80/80 [00:00<00:00, 51646.04it/s]\r\n","Generating training pairs for layer 63\r\n","100%|██████████████████████████████████████| 120/120 [00:00<00:00, 47281.96it/s]\r\n","Generating training pairs for layer 64\r\n","100%|████████████████████████████████████████| 80/80 [00:00<00:00, 51212.50it/s]\r\n","Generating neighbors for layer 0\r\n","100%|██████████████████████████████████| 6376/6376 [00:00<00:00, 1488030.40it/s]\r\n","Generating neighbors for layer 1\r\n","100%|██████████████████████████████████| 6176/6176 [00:00<00:00, 1498554.99it/s]\r\n","Generating neighbors for layer 2\r\n","100%|██████████████████████████████████| 3669/3669 [00:00<00:00, 1499892.92it/s]\r\n","Generating neighbors for layer 3\r\n","100%|██████████████████████████████████| 1550/1550 [00:00<00:00, 1365792.27it/s]\r\n","Generating neighbors for layer 4\r\n","100%|████████████████████████████████████| 853/853 [00:00<00:00, 1299579.12it/s]\r\n","Generating neighbors for layer 5\r\n","100%|████████████████████████████████████| 735/735 [00:00<00:00, 1434533.94it/s]\r\n","Generating neighbors for layer 6\r\n","100%|████████████████████████████████████| 716/716 [00:00<00:00, 1235344.16it/s]\r\n","Generating neighbors for layer 7\r\n","100%|████████████████████████████████████| 705/705 [00:00<00:00, 1385004.37it/s]\r\n","Generating neighbors for layer 8\r\n","100%|████████████████████████████████████| 451/451 [00:00<00:00, 1107512.36it/s]\r\n","Generating neighbors for layer 9\r\n","100%|████████████████████████████████████| 358/358 [00:00<00:00, 1194559.13it/s]\r\n","Generating neighbors for layer 10\r\n","100%|█████████████████████████████████████| 235/235 [00:00<00:00, 993610.32it/s]\r\n","Generating neighbors for layer 11\r\n","100%|█████████████████████████████████████| 206/206 [00:00<00:00, 791958.41it/s]\r\n","Generating neighbors for layer 12\r\n","100%|█████████████████████████████████████| 159/159 [00:00<00:00, 853898.00it/s]\r\n","Generating neighbors for layer 13\r\n","100%|█████████████████████████████████████| 159/159 [00:00<00:00, 963720.14it/s]\r\n","Generating neighbors for layer 14\r\n","100%|█████████████████████████████████████| 122/122 [00:00<00:00, 855694.13it/s]\r\n","Generating neighbors for layer 15\r\n","100%|█████████████████████████████████████| 107/107 [00:00<00:00, 845179.90it/s]\r\n","Generating neighbors for layer 16\r\n","100%|█████████████████████████████████████| 105/105 [00:00<00:00, 884341.20it/s]\r\n","Generating neighbors for layer 17\r\n","100%|█████████████████████████████████████| 103/103 [00:00<00:00, 772832.40it/s]\r\n","Generating neighbors for layer 18\r\n","100%|█████████████████████████████████████| 102/102 [00:00<00:00, 825905.42it/s]\r\n","Generating neighbors for layer 19\r\n","100%|█████████████████████████████████████| 100/100 [00:00<00:00, 820803.13it/s]\r\n","Generating neighbors for layer 20\r\n","100%|███████████████████████████████████████| 81/81 [00:00<00:00, 810832.04it/s]\r\n","Generating neighbors for layer 21\r\n","100%|███████████████████████████████████████| 66/66 [00:00<00:00, 704386.93it/s]\r\n","Generating neighbors for layer 22\r\n","100%|███████████████████████████████████████| 65/65 [00:00<00:00, 663332.75it/s]\r\n","Generating neighbors for layer 23\r\n","100%|███████████████████████████████████████| 61/61 [00:00<00:00, 697145.90it/s]\r\n","Generating neighbors for layer 24\r\n","100%|███████████████████████████████████████| 59/59 [00:00<00:00, 721469.20it/s]\r\n","Generating neighbors for layer 25\r\n","100%|███████████████████████████████████████| 52/52 [00:00<00:00, 528096.39it/s]\r\n","Generating neighbors for layer 26\r\n","100%|███████████████████████████████████████| 50/50 [00:00<00:00, 615000.59it/s]\r\n","Generating neighbors for layer 27\r\n","100%|███████████████████████████████████████| 48/48 [00:00<00:00, 565524.13it/s]\r\n","Generating neighbors for layer 28\r\n","100%|███████████████████████████████████████| 45/45 [00:00<00:00, 504662.25it/s]\r\n","Generating neighbors for layer 29\r\n","100%|███████████████████████████████████████| 43/43 [00:00<00:00, 546530.52it/s]\r\n","Generating neighbors for layer 30\r\n","100%|███████████████████████████████████████| 41/41 [00:00<00:00, 611980.30it/s]\r\n","Generating neighbors for layer 31\r\n","100%|███████████████████████████████████████| 40/40 [00:00<00:00, 553703.50it/s]\r\n","Generating neighbors for layer 32\r\n","100%|███████████████████████████████████████| 38/38 [00:00<00:00, 498073.60it/s]\r\n","Generating neighbors for layer 33\r\n","100%|███████████████████████████████████████| 37/37 [00:00<00:00, 531470.03it/s]\r\n","Generating neighbors for layer 34\r\n","100%|███████████████████████████████████████| 37/37 [00:00<00:00, 540729.09it/s]\r\n","Generating neighbors for layer 35\r\n","100%|███████████████████████████████████████| 35/35 [00:00<00:00, 529966.21it/s]\r\n","Generating neighbors for layer 36\r\n","100%|███████████████████████████████████████| 35/35 [00:00<00:00, 389391.62it/s]\r\n","Generating neighbors for layer 37\r\n","100%|███████████████████████████████████████| 33/33 [00:00<00:00, 477282.87it/s]\r\n","Generating neighbors for layer 38\r\n","100%|███████████████████████████████████████| 31/31 [00:00<00:00, 457828.96it/s]\r\n","Generating neighbors for layer 39\r\n","100%|███████████████████████████████████████| 31/31 [00:00<00:00, 656683.96it/s]\r\n","Generating neighbors for layer 40\r\n","100%|███████████████████████████████████████| 28/28 [00:00<00:00, 553964.68it/s]\r\n","Generating neighbors for layer 41\r\n","100%|███████████████████████████████████████| 28/28 [00:00<00:00, 361355.42it/s]\r\n","Generating neighbors for layer 42\r\n","100%|███████████████████████████████████████| 26/26 [00:00<00:00, 370924.84it/s]\r\n","Generating neighbors for layer 43\r\n","100%|███████████████████████████████████████| 26/26 [00:00<00:00, 340787.20it/s]\r\n","Generating neighbors for layer 44\r\n","100%|███████████████████████████████████████| 26/26 [00:00<00:00, 381300.36it/s]\r\n","Generating neighbors for layer 45\r\n","100%|███████████████████████████████████████| 22/22 [00:00<00:00, 361861.52it/s]\r\n","Generating neighbors for layer 46\r\n","100%|███████████████████████████████████████| 22/22 [00:00<00:00, 386086.56it/s]\r\n","Generating neighbors for layer 47\r\n","100%|███████████████████████████████████████| 13/13 [00:00<00:00, 191319.13it/s]\r\n","Generating neighbors for layer 48\r\n","100%|███████████████████████████████████████| 13/13 [00:00<00:00, 222554.91it/s]\r\n","Generating neighbors for layer 49\r\n","100%|█████████████████████████████████████████| 9/9 [00:00<00:00, 215707.06it/s]\r\n","Generating neighbors for layer 50\r\n","100%|█████████████████████████████████████████| 8/8 [00:00<00:00, 200924.74it/s]\r\n","Generating neighbors for layer 51\r\n","100%|█████████████████████████████████████████| 7/7 [00:00<00:00, 165876.43it/s]\r\n","Generating neighbors for layer 52\r\n","100%|█████████████████████████████████████████| 6/6 [00:00<00:00, 155344.59it/s]\r\n","Generating neighbors for layer 53\r\n","100%|█████████████████████████████████████████| 6/6 [00:00<00:00, 133152.51it/s]\r\n","Generating neighbors for layer 54\r\n","100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 124091.83it/s]\r\n","Generating neighbors for layer 55\r\n","100%|██████████████████████████████████████████| 5/5 [00:00<00:00, 87018.76it/s]\r\n","Generating neighbors for layer 56\r\n","100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 124830.48it/s]\r\n","Generating neighbors for layer 57\r\n","100%|█████████████████████████████████████████| 4/4 [00:00<00:00, 105517.08it/s]\r\n","Generating neighbors for layer 58\r\n","100%|██████████████████████████████████████████| 4/4 [00:00<00:00, 97541.95it/s]\r\n","Generating neighbors for layer 59\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 82241.25it/s]\r\n","Generating neighbors for layer 60\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 77672.30it/s]\r\n","Generating neighbors for layer 61\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 77672.30it/s]\r\n","Generating neighbors for layer 62\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 83886.08it/s]\r\n","Generating neighbors for layer 63\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 83330.54it/s]\r\n","Generating neighbors for layer 64\r\n","100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 84449.07it/s]\r\n","feature dimension: 572\r\n","Optimizing\r\n","Training\r\n","{'epoch': 0, 'iter': 0, 'avg_loss': 17.42611312866211, 'loss': 17.426113}\r\n","{'epoch': 0, 'iter': 5000, 'avg_loss': 11.370488128872353, 'loss': 18.322924}\r\n","{'epoch': 0, 'iter': 10000, 'avg_loss': 8.869661619395998, 'loss': 1.7781382}\r\n","{'epoch': 0, 'iter': 15000, 'avg_loss': 7.399063235282516, 'loss': 2.357699}\r\n","{'epoch': 0, 'iter': 20000, 'avg_loss': 6.402672163224639, 'loss': 3.0305605}\r\n","{'epoch': 0, 'iter': 25000, 'avg_loss': 5.692808632454697, 'loss': 1.647639}\r\n","{'epoch': 0, 'iter': 30000, 'avg_loss': 5.165604598557709, 'loss': 2.2312222}\r\n","{'epoch': 0, 'iter': 35000, 'avg_loss': 4.758965645336136, 'loss': 2.7982535}\r\n","{'epoch': 0, 'iter': 40000, 'avg_loss': 4.433554931267008, 'loss': 1.8237286}\r\n","epoch 0: 100%|| 44222/44222 [08:45<00:00, 84.15it/s]\r\n","valid auc: 0.6911570532634198\r\n","valid pr: 0.7085086586195602\r\n","valid f1: 0.6599825994574253\r\n","Overall ROC-AUC: 0.6887012825307295\r\n","Overall PR-AUC 0.6990539378174906\r\n","Overall F1: 0.6602974707993533\r\n"]}],"source":["!python \"/kaggle/working/GNN_DDI/GNN/main.py\" --input \"/kaggle/working/GNN_DDI/DDI/data5\" --features \"/kaggle/working/GNN_DDI/DDI/data5/featuers_m1.txt\" --epoch 1 --dimensions 32\n"]},{"cell_type":"code","execution_count":6,"id":"616c853c","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:19:42.971305Z","iopub.status.busy":"2024-12-02T23:19:42.970585Z","iopub.status.idle":"2024-12-02T23:19:44.199235Z","shell.execute_reply":"2024-12-02T23:19:44.19821Z"},"papermill":{"duration":1.448581,"end_time":"2024-12-02T23:19:44.20124","exception":false,"start_time":"2024-12-02T23:19:42.752659","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["From https://github.com/OmarKhalifaa/GNN_DDI\r\n"," * branch            main       -> FETCH_HEAD\r\n","Already up to date.\r\n"]}],"source":["!git pull origin main"]},{"cell_type":"code","execution_count":7,"id":"dd63dac3","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:19:44.685428Z","iopub.status.busy":"2024-12-02T23:19:44.684725Z","iopub.status.idle":"2024-12-02T23:19:56.71951Z","shell.execute_reply":"2024-12-02T23:19:56.718659Z"},"papermill":{"duration":12.25547,"end_time":"2024-12-02T23:19:56.721328","exception":false,"start_time":"2024-12-02T23:19:44.465858","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/GNN_DDI/DDI/data5/final_modelss1.csv shape after processing: (37050, 34)\r\n","/kaggle/working/GNN_DDI/DDI/data5/final_modelss2.csv shape after processing: (37050, 34)\r\n","/kaggle/working/GNN_DDI/DDI/data5/final_modelss3.csv shape after processing: (37050, 34)\r\n","/kaggle/working/GNN_DDI/DDI/data5/final_modelss4.csv shape after processing: (37050, 34)\r\n","Reduced data 1 length: 570\r\n","Reduced data 2 length: 570\r\n","Reduced data 3 length: 570\r\n","Reduced data 4 length: 570\r\n","Length of xs1: 570\r\n","Number of feature dictionaries: 4\r\n","First 5 keys in xs1: [0, 1, 2, 3, 4]\r\n","full_pos shape: (37264, 3)\r\n","DDI shape: (37264, 2)\r\n","Multiplying indices 541 and 351: [0.02656227 0.02790939 0.02288867 0.02503109 0.02487968 0.02385757\r\n"," 0.02426968 0.03031032 0.02626601 0.02548944 0.02430399 0.03041907\r\n"," 0.0210613  0.01759591 0.02716357 0.0269257  0.02123744 0.02493482\r\n"," 0.02905957 0.02325038 0.02991576 0.0243281  0.02095308 0.02011403\r\n"," 0.0281166  0.02438699 0.01772582 0.02723098 0.02379801 0.02060952\r\n"," 0.02583191 0.02116332]\r\n","Multiplying indices 541 and 286: [0.0298486  0.02505496 0.02124569 0.02160578 0.02291188 0.02725282\r\n"," 0.02622872 0.02774258 0.02707007 0.02623951 0.02454978 0.02641176\r\n"," 0.02176168 0.016706   0.02468058 0.02699371 0.02031457 0.02495294\r\n"," 0.03187592 0.02448431 0.02817712 0.02257268 0.02038281 0.01798635\r\n"," 0.0299232  0.03188361 0.02070294 0.02778583 0.02144929 0.02252942\r\n"," 0.02807871 0.02343973]\r\n","Multiplying indices 541 and 149: [0.0332038  0.02896088 0.02354496 0.02570142 0.03305005 0.02074411\r\n"," 0.02348606 0.02616058 0.0269374  0.03024961 0.02618999 0.03990289\r\n"," 0.02332601 0.0199756  0.02755325 0.01983554 0.02193956 0.02666219\r\n"," 0.02528155 0.01814226 0.03036592 0.01476323 0.02239094 0.01975857\r\n"," 0.03365525 0.01514438 0.02047737 0.02946364 0.02203275 0.01786932\r\n"," 0.02201791 0.0196534 ]\r\n","Multiplying indices 541 and 280: [0.02419196 0.02667836 0.02308935 0.02365825 0.02475614 0.02570044\r\n"," 0.02389838 0.03055843 0.02649645 0.02646495 0.02266759 0.02919164\r\n"," 0.02073075 0.01588236 0.02633476 0.02851516 0.0239366  0.02158712\r\n"," 0.02819199 0.02335974 0.03338615 0.02561495 0.02079892 0.01774789\r\n"," 0.02659083 0.02854038 0.01566455 0.02720337 0.02413551 0.02315125\r\n"," 0.02837493 0.02310313]\r\n","Multiplying indices 541 and 43: [0.02758421 0.02783309 0.02201351 0.02477616 0.02393216 0.02655521\r\n"," 0.02317812 0.03024953 0.02745185 0.02568114 0.02361489 0.02703386\r\n"," 0.02105369 0.0179015  0.02800685 0.02567607 0.02191914 0.02273181\r\n"," 0.02954613 0.02387679 0.02976817 0.02355174 0.02092724 0.01902483\r\n"," 0.02844194 0.0276934  0.0203965  0.02597411 0.02273247 0.02158253\r\n"," 0.02796099 0.02159565]\r\n","Multiplying indices 541 and 351: [0.01316689 0.01668669 0.01663512 0.03134639 0.03160077 0.01512133\r\n"," 0.02704465 0.00460006 0.01359443 0.02842583 0.00680545 0.01740241\r\n"," 0.01968204 0.02976654 0.01311007 0.02014731 0.04280402 0.01649851\r\n"," 0.01982278 0.01792592 0.00526959 0.02006379 0.01746405 0.0131613\r\n"," 0.02311382 0.02171059 0.02886219 0.0169537  0.02724764 0.01520395\r\n"," 0.03573769 0.0195005 ]\r\n","Multiplying indices 541 and 286: [0.01320031 0.01632574 0.01519586 0.03203774 0.03270405 0.01458473\r\n"," 0.02475905 0.00761368 0.01381238 0.02513058 0.00795987 0.01755921\r\n"," 0.01721826 0.02851976 0.01736302 0.01977807 0.03608815 0.01403248\r\n"," 0.02551701 0.01559846 0.00478074 0.0275172  0.01419013 0.01989709\r\n"," 0.01514783 0.01886086 0.0143143  0.02370042 0.02841098 0.02383904\r\n"," 0.0244606  0.01859596]\r\n","Multiplying indices 541 and 149: [0.01404355 0.02708598 0.00841477 0.01930063 0.04148506 0.0113579\r\n"," 0.02546228 0.00548354 0.00969982 0.023805   0.00443668 0.02104166\r\n"," 0.02896847 0.03382041 0.01109994 0.01566485 0.05315779 0.01926629\r\n"," 0.03165952 0.01028032 0.00284968 0.01677523 0.01383872 0.01566871\r\n"," 0.0228535  0.02561648 0.03043357 0.02670461 0.0341008  0.02342521\r\n"," 0.04113243 0.01859852]\r\n","Multiplying indices 541 and 280: [0.01478571 0.01960068 0.0146465  0.02738729 0.03147595 0.01637646\r\n"," 0.02613558 0.00584221 0.01472161 0.02347025 0.00676857 0.01868729\r\n"," 0.0200228  0.03168362 0.01470177 0.01843572 0.04339745 0.01374619\r\n"," 0.01732981 0.01534435 0.00503731 0.02267224 0.01553814 0.01736174\r\n"," 0.01900107 0.02485912 0.03259467 0.01908293 0.03070026 0.01298917\r\n"," 0.03299975 0.01936498]\r\n","Multiplying indices 541 and 43: [0.01187124 0.02442231 0.01276064 0.02525123 0.03151636 0.012544\r\n"," 0.02468168 0.00478981 0.01262632 0.01959598 0.00583359 0.01688823\r\n"," 0.02036824 0.03225674 0.01469065 0.01905493 0.04174834 0.01075308\r\n"," 0.02413042 0.01080706 0.00418839 0.02387539 0.01403654 0.01107958\r\n"," 0.0211215  0.02231758 0.02927641 0.01876833 0.02768415 0.01987686\r\n"," 0.0297249  0.0162057 ]\r\n","Multiplying indices 541 and 351: [0.02352172 0.02604064 0.02306413 0.02162985 0.01917259 0.01739959\r\n"," 0.02148867 0.02459588 0.02380731 0.03017438 0.02234912 0.0224165\r\n"," 0.0279382  0.02363752 0.02864422 0.02716876 0.02397394 0.02223823\r\n"," 0.02969931 0.01971161 0.03164308 0.02264943 0.03303849 0.0265988\r\n"," 0.02635328 0.02719135 0.0266888  0.01773934 0.02357285 0.02558184\r\n"," 0.02617062 0.02197901]\r\n","Multiplying indices 541 and 286: [0.0238871  0.02706694 0.02329515 0.02100249 0.01949708 0.01967815\r\n"," 0.02167841 0.02490029 0.02606212 0.0297152  0.02035182 0.02304787\r\n"," 0.02547906 0.02194858 0.02975427 0.02761256 0.02336126 0.02212876\r\n"," 0.03333779 0.02278664 0.02755967 0.02187515 0.02838341 0.02493581\r\n"," 0.02392024 0.0268105  0.02981305 0.01875323 0.02404567 0.02988824\r\n"," 0.02494666 0.0236545 ]\r\n","Multiplying indices 541 and 149: [0.02135354 0.02834082 0.02244159 0.01898171 0.02247598 0.02016251\r\n"," 0.02467137 0.01741954 0.02344473 0.02928703 0.01990291 0.01785523\r\n"," 0.0250247  0.02429988 0.0310703  0.02551901 0.0261708  0.02630649\r\n"," 0.03072802 0.02041907 0.03032889 0.02382768 0.03049122 0.02683704\r\n"," 0.02713094 0.02787527 0.03087932 0.01847923 0.02272072 0.02537925\r\n"," 0.02537806 0.02322928]\r\n","Multiplying indices 541 and 280: [0.02597921 0.02758174 0.02433889 0.02135242 0.01889884 0.01827829\r\n"," 0.02029006 0.02625296 0.02506635 0.02949619 0.02038642 0.02254392\r\n"," 0.02709017 0.02433763 0.03010285 0.02532853 0.02467289 0.02050394\r\n"," 0.02974651 0.01909964 0.03040853 0.02312853 0.03428466 0.02524878\r\n"," 0.0263045  0.02453525 0.02574545 0.01720588 0.02300891 0.02718929\r\n"," 0.02527499 0.02134372]\r\n","Multiplying indices 541 and 43: [0.0254064  0.02524703 0.02555433 0.02066205 0.01979849 0.01805406\r\n"," 0.01964753 0.02641337 0.02519187 0.0293951  0.02299049 0.02074571\r\n"," 0.02613585 0.02512604 0.03041677 0.02583323 0.02190951 0.01954013\r\n"," 0.02961772 0.01731456 0.03153052 0.0232197  0.03276174 0.02706047\r\n"," 0.02787473 0.02545553 0.02468661 0.0169776  0.02666539 0.03008776\r\n"," 0.02542222 0.01995136]\r\n","Multiplying indices 541 and 351: [0.02559155 0.0215708  0.0236023  0.01450176 0.02366413 0.01896083\r\n"," 0.0202501  0.02143772 0.01873448 0.01179346 0.02198467 0.02669458\r\n"," 0.03642545 0.03314167 0.02327229 0.01846294 0.01557127 0.01522092\r\n"," 0.03533079 0.0251898  0.02181486 0.02204972 0.02248546 0.03110865\r\n"," 0.01504386 0.01820028 0.01340734 0.02557416 0.0221576  0.02283935\r\n"," 0.01222897 0.03151391]\r\n","Multiplying indices 541 and 286: [0.02080091 0.02282855 0.01712184 0.01285107 0.02447934 0.0191514\r\n"," 0.02796399 0.01751231 0.01822624 0.01335594 0.02120649 0.02263643\r\n"," 0.03286477 0.02840386 0.01915278 0.02340809 0.01949689 0.02736709\r\n"," 0.02784478 0.02540837 0.01792418 0.02253375 0.02061477 0.02314944\r\n"," 0.01607297 0.02154675 0.01817315 0.02349968 0.02011229 0.01942602\r\n"," 0.02189219 0.02956827]\r\n","Multiplying indices 541 and 149: [0.02434547 0.02721503 0.02129434 0.01456214 0.02296421 0.0226863\r\n"," 0.02168987 0.01661385 0.02462966 0.01525774 0.02337159 0.02926\r\n"," 0.0375     0.03348794 0.02717707 0.01840838 0.01037314 0.02136081\r\n"," 0.03465556 0.03158252 0.02722102 0.01412281 0.02177786 0.03567225\r\n"," 0.01269404 0.01558247 0.01205403 0.02442768 0.02409741 0.01842605\r\n"," 0.01987329 0.02267691]\r\n","Multiplying indices 541 and 280: [0.02342584 0.02114053 0.01860392 0.01178135 0.02464585 0.01814776\r\n"," 0.020836   0.02334332 0.02081381 0.01353683 0.02280624 0.02185091\r\n"," 0.03164467 0.02645226 0.02942009 0.01980739 0.0162871  0.020978\r\n"," 0.02662489 0.02677222 0.02348369 0.02431045 0.02299339 0.03163283\r\n"," 0.01449662 0.0176347  0.01337481 0.02509134 0.01747865 0.02136388\r\n"," 0.01685216 0.02424517]\r\n","Multiplying indices 541 and 43: [0.01987333 0.01877806 0.02404355 0.01552509 0.02874403 0.01940602\r\n"," 0.02442901 0.01749294 0.02329076 0.01564647 0.02248937 0.02178738\r\n"," 0.03046645 0.02336622 0.02633951 0.01996081 0.0161217  0.0195575\r\n"," 0.03466501 0.02593987 0.02523388 0.02001286 0.01918775 0.03749618\r\n"," 0.01504088 0.01586047 0.0133081  0.02493977 0.01752009 0.02350206\r\n"," 0.01228272 0.02548849]\r\n","Saved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt with shape (37264, 32)\r\n","Saved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt with shape (37264, 32)\r\n","Saved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt with shape (37264, 32)\r\n","Saved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt with shape (37264, 32)\r\n","Cleanup completed. Large variables have been deleted.\r\n"]}],"source":["!python concat.py\n"]},{"cell_type":"code","execution_count":8,"id":"47a7ee2d","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:19:57.157226Z","iopub.status.busy":"2024-12-02T23:19:57.156387Z","iopub.status.idle":"2024-12-02T23:19:58.365141Z","shell.execute_reply":"2024-12-02T23:19:58.364293Z"},"papermill":{"duration":1.428123,"end_time":"2024-12-02T23:19:58.367144","exception":false,"start_time":"2024-12-02T23:19:56.939021","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["From https://github.com/OmarKhalifaa/GNN_DDI\r\n"," * branch            main       -> FETCH_HEAD\r\n","Already up to date.\r\n"]}],"source":["!git pull origin main"]},{"cell_type":"code","execution_count":9,"id":"91680313","metadata":{"execution":{"iopub.execute_input":"2024-12-02T23:19:58.811358Z","iopub.status.busy":"2024-12-02T23:19:58.810675Z","iopub.status.idle":"2024-12-02T23:31:20.089161Z","shell.execute_reply":"2024-12-02T23:31:20.088032Z"},"papermill":{"duration":681.501461,"end_time":"2024-12-02T23:31:20.091331","exception":false,"start_time":"2024-12-02T23:19:58.58987","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded full_pos2.txt with shape: (37264, 3)\r\n","new_label: 37264, first label: 1\r\n","Cross-Validation Fold 1/5\r\n","Processing Feature 1/4: 1\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n","I0000 00:00:1733181606.185564    1467 service.cc:145] XLA service 0x7dbb9000af80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\r\n","I0000 00:00:1733181606.185614    1467 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\r\n","I0000 00:00:1733181606.185625    1467 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\r\n","I0000 00:00:1733181609.639652    1467 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\r\n","Processing Feature 2/4: 2\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Processing Feature 3/4: 3\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Processing Feature 4/4: 4\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","Cross-Validation Fold 2/5\r\n","Processing Feature 1/4: 1\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","Processing Feature 2/4: 2\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Processing Feature 3/4: 3\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Processing Feature 4/4: 4\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","Cross-Validation Fold 3/5\r\n","Processing Feature 1/4: 1\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","Processing Feature 2/4: 2\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Processing Feature 3/4: 3\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Processing Feature 4/4: 4\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","Cross-Validation Fold 4/5\r\n","Processing Feature 1/4: 1\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","Processing Feature 2/4: 2\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Processing Feature 3/4: 3\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Processing Feature 4/4: 4\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","Cross-Validation Fold 5/5\r\n","Processing Feature 1/4: 1\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","Processing Feature 2/4: 2\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Processing Feature 3/4: 3\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Processing Feature 4/4: 4\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\r\n","  _warn_prf(average, modifier, msg_start, len(result))\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\r\n","  _warn_prf(average, modifier, msg_start, len(result))\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\r\n","  _warn_prf(average, modifier, msg_start, len(result))\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\r\n","  _warn_prf(average, modifier, msg_start, len(result))\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\r\n","  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\r\n","Overall Evaluation Results:\r\n"," [[0.76865071]\r\n"," [0.83156628]\r\n"," [0.76432392]\r\n"," [0.99546298]\r\n"," [0.98093222]\r\n"," [0.76865071]\r\n"," [0.61939628]\r\n"," [0.76865071]\r\n"," [0.81959631]\r\n"," [0.76865071]\r\n"," [0.53858793]]\r\n","Per-Event Evaluation Results:\r\n"," [[0.88221876 0.85049937 0.94690998 0.79364333 0.73653897 0.86034659]\r\n"," [0.89139652 0.90428038 0.95774617 0.80828083 0.73460777 0.89837826]\r\n"," [0.92864427 0.80574345 0.93976837 0.75391023 0.78949409 0.72139568]\r\n"," [0.97767282 0.87194866 0.97796769 0.82068966 0.8447205  0.79798826]\r\n"," [0.98532095 0.83154982 0.98169716 0.76772824 0.86673058 0.68902439]\r\n"," [0.98212752 0.83609848 0.99182481 0.61547344 0.88833333 0.47084806]\r\n"," [0.9782632  0.67161706 0.9786931  0.48669202 0.80672269 0.34845735]\r\n"," [0.98762881 0.85954914 0.9884915  0.75952008 0.87605295 0.67034991]\r\n"," [0.98668957 0.68323071 0.99144029 0.66304348 0.62805663 0.70215827]\r\n"," [0.99398884 0.90312469 0.99626474 0.76421053 0.90977444 0.65880218]\r\n"," [0.99651138 0.88310578 0.99686275 0.79099678 0.94615385 0.67955801]\r\n"," [0.99694075 0.87152199 0.99761993 0.82131661 0.81875    0.82389937]\r\n"," [0.99476707 0.57375525 0.96711628 0.49350649 0.67857143 0.3877551 ]\r\n"," [0.99699442 0.81534423 0.99431445 0.71717172 0.94039735 0.57959184]\r\n"," [0.99608201 0.60647209 0.99239099 0.43846154 0.79166667 0.30319149]\r\n"," [0.99814835 0.87592304 0.99108168 0.74538745 0.95283019 0.61212121]\r\n"," [0.99822885 0.96128734 0.99969475 0.7480916  0.98989899 0.60122699]\r\n"," [0.99686024 0.66229978 0.99358543 0.53386454 0.72826087 0.42138365]\r\n"," [0.99865822 0.90085713 0.99958057 0.83870968 0.85526316 0.82278481]\r\n"," [0.9995438  0.97634184 0.99986404 0.94462541 0.94771242 0.94155844]\r\n"," [0.99868506 0.83918559 0.99821665 0.7860262  0.87378641 0.71428571]\r\n"," [0.9983362  0.81698419 0.99919932 0.60759494 0.85714286 0.47058824]\r\n"," [0.99855088 0.7703249  0.99792353 0.65822785 0.89655172 0.52      ]\r\n"," [0.99785316 0.57493864 0.99822829 0.35483871 0.75862069 0.23157895]\r\n"," [0.99779948 0.5997984  0.99821921 0.19607843 1.         0.10869565]\r\n"," [0.9992486  0.85539772 0.99919484 0.81333333 0.88405797 0.75308642]\r\n"," [0.99844354 0.58041726 0.97826115 0.49122807 0.75675676 0.36363636]\r\n"," [0.99871189 0.71434158 0.99770757 0.57142857 0.86486486 0.42666667]\r\n"," [0.99884607 0.74124667 0.98827768 0.58252427 0.90909091 0.42857143]\r\n"," [0.99900708 0.8104244  0.99950445 0.75167785 0.68292683 0.8358209 ]\r\n"," [0.99922177 0.7903459  0.99879662 0.73394495 0.88888889 0.625     ]\r\n"," [0.99900708 0.78643535 0.99972643 0.63366337 0.82051282 0.51612903]\r\n"," [0.99852404 0.43498122 0.99661108 0.22535211 0.66666667 0.13559322]\r\n"," [0.99903392 0.77403307 0.99925299 0.57142857 0.92307692 0.4137931 ]\r\n"," [0.99970481 0.98157912 0.99996416 0.89719626 0.96       0.84210526]\r\n"," [0.99932911 0.8603274  0.99980748 0.75247525 0.82608696 0.69090909]\r\n"," [0.99973164 0.99546388 0.99999353 0.89795918 1.         0.81481481]\r\n"," [0.99927544 0.78021044 0.9972527  0.6746988  0.875      0.54901961]\r\n"," [0.99868506 0.1722695  0.96478159 0.         0.         0.        ]\r\n"," [0.99946329 0.85170777 0.99906906 0.78723404 0.80434783 0.77083333]\r\n"," [0.99983899 0.94087938 0.99871159 0.92682927 1.         0.86363636]\r\n"," [0.99940962 0.86679883 0.99973011 0.73170732 0.78947368 0.68181818]\r\n"," [0.99922177 0.49112305 0.99806442 0.56716418 0.7037037  0.475     ]\r\n"," [0.99908759 0.63330669 0.99662785 0.34615385 0.75       0.225     ]\r\n"," [0.9996243  0.92893128 0.99984754 0.78787879 1.         0.65      ]\r\n"," [0.9992486  0.55259708 0.99821697 0.3        1.         0.17647059]\r\n"," [0.99943645 0.83414582 0.99984279 0.6440678  0.76       0.55882353]\r\n"," [0.99959747 0.67436533 0.92111525 0.44444444 1.         0.28571429]\r\n"," [0.99981215 0.83325722 0.999561   0.78787879 1.         0.65      ]\r\n"," [0.99965114 0.71739434 0.99931452 0.23529412 1.         0.13333333]\r\n"," [0.99994633 0.91694156 0.92996161 0.91666667 1.         0.84615385]\r\n"," [0.99967797 0.19681507 0.85875031 0.         0.         0.        ]\r\n"," [0.99997316 1.         1.         0.94736842 1.         0.9       ]\r\n"," [0.99986582 0.95492063 0.99998389 0.66666667 1.         0.5       ]\r\n"," [0.99986582 0.95784231 0.99998807 0.61538462 1.         0.44444444]\r\n"," [0.99975848 0.5104579  0.99925141 0.         0.         0.        ]\r\n"," [0.99991949 0.76618318 0.99988368 0.82352941 0.875      0.77777778]\r\n"," [0.99986582 0.7206882  0.8571697  0.54545455 0.75       0.42857143]\r\n"," [0.99983899 0.709453   0.99971626 0.25       1.         0.14285714]\r\n"," [0.99994633 1.         1.         0.8        1.         0.66666667]\r\n"," [0.99991949 0.87464286 0.99997853 0.72727273 0.66666667 0.8       ]\r\n"," [1.         1.         1.         1.         1.         1.        ]\r\n"," [0.99991949 0.77666667 0.99997853 0.57142857 1.         0.4       ]\r\n"," [0.99983899 0.05327496 0.61207225 0.         0.         0.        ]\r\n"," [0.99989266 0.69540598 0.99992485 0.33333333 1.         0.2       ]]\r\n","Saved all results to /kaggle/working/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL.csv\r\n","Saved each results to /kaggle/working/GNN_DDI/DDI/data5/G_allf_32_cm_each_DDIMDL.csv\r\n","\r\n","Training final model on all data...\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\r\n","Loaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\r\n","Final feature matrix shape: (37264, 128)\r\n","Final DNN model architecture:\r\n","\u001b[1mModel: \"functional_41\"\u001b[0m\r\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\r\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\r\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\r\n","│ Inputlayer (\u001b[94mInputLayer\u001b[0m)         │ (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)            │             \u001b[32m0\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ dense_60 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │        \u001b[32m66,048\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ batch_normalization_40          │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │         \u001b[32m2,048\u001b[0m │\r\n","│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ dropout_40 (\u001b[94mDropout\u001b[0m)            │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │             \u001b[32m0\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ dense_61 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │       \u001b[32m131,328\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ batch_normalization_41          │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │         \u001b[32m1,024\u001b[0m │\r\n","│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ dropout_41 (\u001b[94mDropout\u001b[0m)            │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │             \u001b[32m0\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ dense_62 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m65\u001b[0m)             │        \u001b[32m16,705\u001b[0m │\r\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\r\n","│ activation_20 (\u001b[94mActivation\u001b[0m)      │ (\u001b[96mNone\u001b[0m, \u001b[32m65\u001b[0m)             │             \u001b[32m0\u001b[0m │\r\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\r\n","\u001b[1m Total params: \u001b[0m\u001b[32m217,153\u001b[0m (848.25 KB)\r\n","\u001b[1m Trainable params: \u001b[0m\u001b[32m215,617\u001b[0m (842.25 KB)\r\n","\u001b[1m Non-trainable params: \u001b[0m\u001b[32m1,536\u001b[0m (6.00 KB)\r\n","Epoch 1/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4445 - loss: 2.5873 - val_accuracy: 0.2427 - val_loss: 2.6068\r\n","Epoch 2/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6592 - loss: 1.1900 - val_accuracy: 0.2433 - val_loss: 2.2548\r\n","Epoch 3/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7011 - loss: 0.9654 - val_accuracy: 0.6321 - val_loss: 1.1016\r\n","Epoch 4/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7281 - loss: 0.8721 - val_accuracy: 0.6285 - val_loss: 1.2634\r\n","Epoch 5/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7449 - loss: 0.7965 - val_accuracy: 0.7329 - val_loss: 0.8580\r\n","Epoch 6/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7570 - loss: 0.7501 - val_accuracy: 0.5541 - val_loss: 1.2544\r\n","Epoch 7/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7682 - loss: 0.7071 - val_accuracy: 0.6400 - val_loss: 1.2210\r\n","Epoch 8/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.6744 - val_accuracy: 0.7158 - val_loss: 0.9072\r\n","Epoch 9/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.6522 - val_accuracy: 0.7194 - val_loss: 0.9245\r\n","Epoch 10/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7905 - loss: 0.6221 - val_accuracy: 0.7518 - val_loss: 0.7601\r\n","Epoch 11/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7922 - loss: 0.6166 - val_accuracy: 0.7440 - val_loss: 0.8066\r\n","Epoch 12/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7977 - loss: 0.6072 - val_accuracy: 0.7608 - val_loss: 0.7291\r\n","Epoch 13/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8025 - loss: 0.5848 - val_accuracy: 0.7020 - val_loss: 0.9152\r\n","Epoch 14/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8107 - loss: 0.5717 - val_accuracy: 0.7879 - val_loss: 0.6481\r\n","Epoch 15/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8146 - loss: 0.5499 - val_accuracy: 0.7782 - val_loss: 0.6639\r\n","Epoch 16/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8121 - loss: 0.5554 - val_accuracy: 0.7008 - val_loss: 0.9196\r\n","Epoch 17/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8164 - loss: 0.5456 - val_accuracy: 0.7445 - val_loss: 0.7974\r\n","Epoch 18/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8174 - loss: 0.5394 - val_accuracy: 0.7770 - val_loss: 0.6686\r\n","Epoch 19/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8179 - loss: 0.5413 - val_accuracy: 0.7742 - val_loss: 0.6842\r\n","Epoch 20/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8213 - loss: 0.5294 - val_accuracy: 0.7702 - val_loss: 0.7105\r\n","Epoch 21/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8181 - loss: 0.5331 - val_accuracy: 0.7935 - val_loss: 0.6316\r\n","Epoch 22/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8284 - loss: 0.5104 - val_accuracy: 0.7810 - val_loss: 0.6713\r\n","Epoch 23/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8292 - loss: 0.5024 - val_accuracy: 0.8162 - val_loss: 0.5455\r\n","Epoch 24/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8295 - loss: 0.5075 - val_accuracy: 0.7770 - val_loss: 0.6975\r\n","Epoch 25/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8327 - loss: 0.4859 - val_accuracy: 0.7643 - val_loss: 0.7436\r\n","Epoch 26/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8296 - loss: 0.4952 - val_accuracy: 0.7554 - val_loss: 0.7529\r\n","Epoch 27/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8341 - loss: 0.4857 - val_accuracy: 0.7863 - val_loss: 0.6582\r\n","Epoch 28/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8303 - loss: 0.4923 - val_accuracy: 0.7953 - val_loss: 0.6279\r\n","Epoch 29/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8343 - loss: 0.4832 - val_accuracy: 0.7813 - val_loss: 0.6854\r\n","Epoch 30/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8398 - loss: 0.4683 - val_accuracy: 0.7651 - val_loss: 0.8150\r\n","Epoch 31/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8390 - loss: 0.4737 - val_accuracy: 0.8194 - val_loss: 0.5329\r\n","Epoch 32/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8383 - loss: 0.4629 - val_accuracy: 0.8068 - val_loss: 0.5755\r\n","Epoch 33/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8406 - loss: 0.4601 - val_accuracy: 0.8300 - val_loss: 0.5211\r\n","Epoch 34/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.4637 - val_accuracy: 0.7910 - val_loss: 0.6165\r\n","Epoch 35/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8413 - loss: 0.4652 - val_accuracy: 0.7758 - val_loss: 0.6784\r\n","Epoch 36/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8425 - loss: 0.4568 - val_accuracy: 0.8064 - val_loss: 0.5716\r\n","Epoch 37/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8489 - loss: 0.4389 - val_accuracy: 0.8081 - val_loss: 0.5596\r\n","Epoch 38/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8402 - loss: 0.4576 - val_accuracy: 0.7594 - val_loss: 0.6877\r\n","Epoch 39/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8459 - loss: 0.4490 - val_accuracy: 0.8143 - val_loss: 0.5660\r\n","Epoch 40/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8514 - loss: 0.4389 - val_accuracy: 0.8068 - val_loss: 0.5825\r\n","Epoch 41/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8431 - loss: 0.4534 - val_accuracy: 0.8197 - val_loss: 0.5353\r\n","Epoch 42/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8509 - loss: 0.4341 - val_accuracy: 0.8127 - val_loss: 0.5640\r\n","Epoch 43/100\r\n","\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8495 - loss: 0.4373 - val_accuracy: 0.8089 - val_loss: 0.5580\r\n","Epoch 43: early stopping\r\n","Final model saved to /kaggle/working/GNN_DDI/DDI/model_trained_final.h5\r\n","\r\n","Time used: 933.34 seconds\r\n"]}],"source":["!python train.py\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":6205975,"sourceId":10069096,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":1380.271144,"end_time":"2024-12-02T23:31:20.593843","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-02T23:08:20.322699","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}