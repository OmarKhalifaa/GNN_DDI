{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10069096,"sourceType":"datasetVersion","datasetId":6205975}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":1380.271144,"end_time":"2024-12-02T23:31:20.593843","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-02T23:08:20.322699","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mahmmoudsalah/ddi-drugsafe-output?scriptVersionId=211078477\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"6ed71ca9","cell_type":"code","source":"!git clone --branch main https://github.com/OmarKhalifaa/GNN_DDI.git\n\n ","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:44:08.467554Z","iopub.execute_input":"2024-12-03T19:44:08.467924Z","iopub.status.idle":"2024-12-03T19:44:12.342963Z","shell.execute_reply.started":"2024-12-03T19:44:08.467873Z","shell.execute_reply":"2024-12-03T19:44:12.342038Z"},"papermill":{"duration":3.897507,"end_time":"2024-12-02T23:08:26.54661","exception":false,"start_time":"2024-12-02T23:08:22.649103","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'GNN_DDI'...\nremote: Enumerating objects: 166, done.\u001b[K\nremote: Counting objects: 100% (164/164), done.\u001b[K\nremote: Compressing objects: 100% (132/132), done.\u001b[K\nremote: Total 166 (delta 77), reused 84 (delta 29), pack-reused 2 (from 1)\u001b[K\nReceiving objects: 100% (166/166), 43.66 MiB | 26.93 MiB/s, done.\nResolving deltas: 100% (77/77), done.\n","output_type":"stream"}],"execution_count":1},{"id":"ce72c992","cell_type":"code","source":"%cd /kaggle/working/GNN_DDI\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:44:12.344756Z","iopub.execute_input":"2024-12-03T19:44:12.345031Z","iopub.status.idle":"2024-12-03T19:44:12.352085Z","shell.execute_reply.started":"2024-12-03T19:44:12.345003Z","shell.execute_reply":"2024-12-03T19:44:12.351134Z"},"papermill":{"duration":0.012003,"end_time":"2024-12-02T23:08:26.562111","exception":false,"start_time":"2024-12-02T23:08:26.550108","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/GNN_DDI\n","output_type":"stream"}],"execution_count":2},{"id":"999b8f31","cell_type":"code","source":"!python prepare.py","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:44:12.353132Z","iopub.execute_input":"2024-12-03T19:44:12.353348Z","iopub.status.idle":"2024-12-03T19:44:15.926612Z","shell.execute_reply.started":"2024-12-03T19:44:12.353326Z","shell.execute_reply":"2024-12-03T19:44:15.925538Z"},"papermill":{"duration":4.527511,"end_time":"2024-12-02T23:08:31.093364","exception":false,"start_time":"2024-12-02T23:08:26.565853","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Tables :  [('event_number',), ('event',), ('drug',), ('extraction',)]\ndrug :  [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\nevent :  [('index',), ('id1',), ('name1',), ('id2',), ('name2',), ('interaction',)]\nevent row COUNT:  [(37264,)]\n0    P14780|Q00653|P01375|P01579|P33673\n1                                Q02641\nName: target, dtype: object\n   index  ...          name\n0      0  ...   Glucosamine\n1      1  ...  Azelnidipine\n\n[2 rows x 7 columns]\n/kaggle/working/GNN_DDI/prepare.py:53: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\nYou are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\nA typical example is when you are setting values in a column of a DataFrame, like:\n\ndf[\"col\"][row_indexer] = value\n\nUse `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n  df_feature[each_feature].iloc[i] = 1\nTraceback (most recent call last):\n  File \"/kaggle/working/GNN_DDI/prepare.py\", line 76, in <module>\n    mat = feature_vector(df_drug, feature)\n  File \"/kaggle/working/GNN_DDI/prepare.py\", line 58, in feature_vector\n    pca.fit(sim_matrix)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\", line 435, in fit\n    self._fit(X)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/decomposition/_pca.py\", line 485, in _fit\n    X = self._validate_data(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 565, in _validate_data\n    X = check_array(X, input_name=\"X\", **check_params)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 737, in check_array\n    raise TypeError(\nTypeError: np.matrix is not supported. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n","output_type":"stream"}],"execution_count":3},{"id":"1da9ec20","cell_type":"code","source":"!git pull origin main","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:44:15.928863Z","iopub.execute_input":"2024-12-03T19:44:15.929155Z","iopub.status.idle":"2024-12-03T19:44:17.069112Z","shell.execute_reply.started":"2024-12-03T19:44:15.929128Z","shell.execute_reply":"2024-12-03T19:44:17.068323Z"},"papermill":{"duration":1.20407,"end_time":"2024-12-02T23:08:32.301292","exception":false,"start_time":"2024-12-02T23:08:31.097222","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"From https://github.com/OmarKhalifaa/GNN_DDI\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":4},{"id":"1639f297","cell_type":"code","source":"!python \"/kaggle/working/GNN_DDI/GNN/main.py\" --input \"/kaggle/working/GNN_DDI/DDI/data5\" --features \"/kaggle/working/GNN_DDI/DDI/data5/featuers_m1.txt\" --epoch 1 --dimensions 32\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:44:17.07028Z","iopub.execute_input":"2024-12-03T19:44:17.070534Z","iopub.status.idle":"2024-12-03T19:55:34.803095Z","shell.execute_reply.started":"2024-12-03T19:44:17.070509Z","shell.execute_reply":"2024-12-03T19:55:34.801947Z"},"papermill":{"duration":670.228418,"end_time":"2024-12-02T23:19:42.533241","exception":false,"start_time":"2024-12-02T23:08:32.304823","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Namespace(input='/kaggle/working/GNN_DDI/DDI/data5', features='/kaggle/working/GNN_DDI/DDI/data5/featuers_m1.txt', walk_file=None, epoch=1, batch_size=64, eval_type='all', schema=None, dimensions=32, edge_dim=10, att_dim=20, walk_length=10, num_walks=20, window_size=5, negative_samples=5, neighbor_samples=10, patience=5, num_workers=16, file=None)\nWe are loading data from: /kaggle/working/GNN_DDI/DDI/data5/train.txt\nTotal training nodes: 570\nWe are loading data from: /kaggle/working/GNN_DDI/DDI/data5/valid.txt\nWe are loading data from: /kaggle/working/GNN_DDI/DDI/data5/test.txt\nGenerating random walks for layer 0\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n7940it [00:00, 124179.75it/s]\nGenerating random walks for layer 1\n8980it [00:00, 42677.59it/s]\nGenerating random walks for layer 2\n9640it [00:00, 48609.09it/s]\nGenerating random walks for layer 3\n8440it [00:00, 128891.98it/s]\nGenerating random walks for layer 4\n6100it [00:00, 285581.59it/s]\nGenerating random walks for layer 5\n2220it [00:00, 465218.83it/s]\nGenerating random walks for layer 6\n1680it [00:00, 461516.29it/s]\nGenerating random walks for layer 7\n2000it [00:00, 372644.84it/s]\nGenerating random walks for layer 8\n5400it [00:00, 472193.67it/s]\nGenerating random walks for layer 9\n1600it [00:00, 410702.96it/s]\nGenerating random walks for layer 10\n1040it [00:00, 337674.27it/s]\nGenerating random walks for layer 11\n1620it [00:00, 466097.71it/s]\nGenerating random walks for layer 12\n2900it [00:00, 476345.47it/s]\nGenerating random walks for layer 13\n740it [00:00, 282701.97it/s]\nGenerating random walks for layer 14\n980it [00:00, 373878.29it/s]\nGenerating random walks for layer 15\n1020it [00:00, 360998.23it/s]\nGenerating random walks for layer 16\n680it [00:00, 273139.89it/s]\nGenerating random walks for layer 17\n1060it [00:00, 332011.22it/s]\nGenerating random walks for layer 18\n1340it [00:00, 423443.63it/s]\nGenerating random walks for layer 19\n880it [00:00, 283224.95it/s]\nGenerating random walks for layer 20\n460it [00:00, 210332.48it/s]\nGenerating random walks for layer 21\n620it [00:00, 254723.13it/s]\nGenerating random walks for layer 22\n840it [00:00, 288835.49it/s]\nGenerating random walks for layer 23\n440it [00:00, 176298.60it/s]\nGenerating random walks for layer 24\n300it [00:00, 121117.64it/s]\nGenerating random walks for layer 25\n800it [00:00, 279620.27it/s]\nGenerating random walks for layer 26\n880it [00:00, 308275.91it/s]\nGenerating random walks for layer 27\n980it [00:00, 353706.04it/s]\nGenerating random walks for layer 28\n680it [00:00, 228005.97it/s]\nGenerating random walks for layer 29\n380it [00:00, 162387.72it/s]\nGenerating random walks for layer 30\n480it [00:00, 165197.83it/s]\nGenerating random walks for layer 31\n660it [00:00, 238888.56it/s]\nGenerating random walks for layer 32\n560it [00:00, 206870.73it/s]\nGenerating random walks for layer 33\n420it [00:00, 122589.26it/s]\nGenerating random walks for layer 34\n560it [00:00, 168906.24it/s]\nGenerating random walks for layer 35\n520it [00:00, 222759.48it/s]\nGenerating random walks for layer 36\n420it [00:00, 179134.40it/s]\nGenerating random walks for layer 37\n420it [00:00, 189115.16it/s]\nGenerating random walks for layer 38\n640it [00:00, 256728.63it/s]\nGenerating random walks for layer 39\n620it [00:00, 236170.06it/s]\nGenerating random walks for layer 40\n580it [00:00, 217749.40it/s]\nGenerating random walks for layer 41\n360it [00:00, 159159.84it/s]\nGenerating random walks for layer 42\n380it [00:00, 169089.28it/s]\nGenerating random walks for layer 43\n540it [00:00, 227653.45it/s]\nGenerating random walks for layer 44\n260it [00:00, 110836.37it/s]\nGenerating random walks for layer 45\n400it [00:00, 183799.47it/s]\nGenerating random walks for layer 46\n300it [00:00, 133378.33it/s]\nGenerating random walks for layer 47\n260it [00:00, 118896.54it/s]\nGenerating random walks for layer 48\n280it [00:00, 115443.34it/s]\nGenerating random walks for layer 49\n160it [00:00, 413231.92it/s]\nGenerating random walks for layer 50\n200it [00:00, 284070.71it/s]\nGenerating random walks for layer 51\n220it [00:00, 317531.62it/s]\nGenerating random walks for layer 52\n120it [00:00, 321813.61it/s]\nGenerating random walks for layer 53\n120it [00:00, 337117.54it/s]\nGenerating random walks for layer 54\n120it [00:00, 351232.71it/s]\nGenerating random walks for layer 55\n140it [00:00, 367691.02it/s]\nGenerating random walks for layer 56\n120it [00:00, 180659.18it/s]\nGenerating random walks for layer 57\n120it [00:00, 341463.01it/s]\nGenerating random walks for layer 58\n100it [00:00, 138380.20it/s]\nGenerating random walks for layer 59\n80it [00:00, 227025.93it/s]\nGenerating random walks for layer 60\n80it [00:00, 227025.93it/s]\nGenerating random walks for layer 61\n80it [00:00, 238651.72it/s]\nGenerating random walks for layer 62\n80it [00:00, 124321.72it/s]\nGenerating random walks for layer 63\n120it [00:00, 176107.94it/s]\nGenerating random walks for layer 64\n80it [00:00, 209715.20it/s]\nFinish generating the walks\nSaving walks for layer 0\n100%|███████████████████████████████████| 7940/7940 [00:00<00:00, 504106.29it/s]\nSaving walks for layer 1\n100%|███████████████████████████████████| 8980/8980 [00:00<00:00, 496511.29it/s]\nSaving walks for layer 2\n100%|███████████████████████████████████| 9640/9640 [00:00<00:00, 512960.56it/s]\nSaving walks for layer 3\n100%|███████████████████████████████████| 8440/8440 [00:00<00:00, 522038.10it/s]\nSaving walks for layer 4\n100%|███████████████████████████████████| 6100/6100 [00:00<00:00, 504600.32it/s]\nSaving walks for layer 5\n100%|███████████████████████████████████| 2220/2220 [00:00<00:00, 538882.74it/s]\nSaving walks for layer 6\n100%|███████████████████████████████████| 1680/1680 [00:00<00:00, 551536.53it/s]\nSaving walks for layer 7\n100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 553703.50it/s]\nSaving walks for layer 8\n100%|███████████████████████████████████| 5400/5400 [00:00<00:00, 552960.00it/s]\nSaving walks for layer 9\n100%|███████████████████████████████████| 1600/1600 [00:00<00:00, 544273.03it/s]\nSaving walks for layer 10\n100%|███████████████████████████████████| 1040/1040 [00:00<00:00, 566356.29it/s]\nSaving walks for layer 11\n100%|███████████████████████████████████| 1620/1620 [00:00<00:00, 572528.86it/s]\nSaving walks for layer 12\n100%|███████████████████████████████████| 2900/2900 [00:00<00:00, 563541.59it/s]\nSaving walks for layer 13\n100%|█████████████████████████████████████| 740/740 [00:00<00:00, 570129.49it/s]\nSaving walks for layer 14\n100%|█████████████████████████████████████| 980/980 [00:00<00:00, 539708.24it/s]\nSaving walks for layer 15\n100%|███████████████████████████████████| 1020/1020 [00:00<00:00, 437935.31it/s]\nSaving walks for layer 16\n100%|█████████████████████████████████████| 680/680 [00:00<00:00, 503198.08it/s]\nSaving walks for layer 17\n100%|███████████████████████████████████| 1060/1060 [00:00<00:00, 526149.38it/s]\nSaving walks for layer 18\n100%|███████████████████████████████████| 1340/1340 [00:00<00:00, 541774.37it/s]\nSaving walks for layer 19\n100%|█████████████████████████████████████| 880/880 [00:00<00:00, 531383.17it/s]\nSaving walks for layer 20\n100%|█████████████████████████████████████| 460/460 [00:00<00:00, 478873.13it/s]\nSaving walks for layer 21\n100%|█████████████████████████████████████| 620/620 [00:00<00:00, 502214.85it/s]\nSaving walks for layer 22\n100%|█████████████████████████████████████| 840/840 [00:00<00:00, 523976.11it/s]\nSaving walks for layer 23\n100%|█████████████████████████████████████| 440/440 [00:00<00:00, 469591.29it/s]\nSaving walks for layer 24\n100%|█████████████████████████████████████| 300/300 [00:00<00:00, 424811.34it/s]\nSaving walks for layer 25\n100%|█████████████████████████████████████| 800/800 [00:00<00:00, 509790.82it/s]\nSaving walks for layer 26\n100%|█████████████████████████████████████| 880/880 [00:00<00:00, 530390.50it/s]\nSaving walks for layer 27\n100%|█████████████████████████████████████| 980/980 [00:00<00:00, 527044.23it/s]\nSaving walks for layer 28\n100%|█████████████████████████████████████| 680/680 [00:00<00:00, 491661.22it/s]\nSaving walks for layer 29\n100%|█████████████████████████████████████| 380/380 [00:00<00:00, 466170.08it/s]\nSaving walks for layer 30\n100%|█████████████████████████████████████| 480/480 [00:00<00:00, 488301.22it/s]\nSaving walks for layer 31\n100%|█████████████████████████████████████| 660/660 [00:00<00:00, 504141.44it/s]\nSaving walks for layer 32\n100%|█████████████████████████████████████| 560/560 [00:00<00:00, 483792.02it/s]\nSaving walks for layer 33\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 461274.60it/s]\nSaving walks for layer 34\n100%|█████████████████████████████████████| 560/560 [00:00<00:00, 484590.52it/s]\nSaving walks for layer 35\n100%|█████████████████████████████████████| 520/520 [00:00<00:00, 490782.65it/s]\nSaving walks for layer 36\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 482764.51it/s]\nSaving walks for layer 37\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 477141.84it/s]\nSaving walks for layer 38\n100%|█████████████████████████████████████| 640/640 [00:00<00:00, 490651.54it/s]\nSaving walks for layer 39\n100%|█████████████████████████████████████| 620/620 [00:00<00:00, 487252.85it/s]\nSaving walks for layer 40\n100%|█████████████████████████████████████| 580/580 [00:00<00:00, 500246.00it/s]\nSaving walks for layer 41\n100%|█████████████████████████████████████| 360/360 [00:00<00:00, 457699.13it/s]\nSaving walks for layer 42\n100%|█████████████████████████████████████| 380/380 [00:00<00:00, 465353.44it/s]\nSaving walks for layer 43\n100%|█████████████████████████████████████| 540/540 [00:00<00:00, 507716.69it/s]\nSaving walks for layer 44\n100%|█████████████████████████████████████| 260/260 [00:00<00:00, 431205.63it/s]\nSaving walks for layer 45\n100%|█████████████████████████████████████| 400/400 [00:00<00:00, 488419.68it/s]\nSaving walks for layer 46\n100%|█████████████████████████████████████| 300/300 [00:00<00:00, 456398.69it/s]\nSaving walks for layer 47\n100%|█████████████████████████████████████| 260/260 [00:00<00:00, 423667.07it/s]\nSaving walks for layer 48\n100%|█████████████████████████████████████| 280/280 [00:00<00:00, 451347.09it/s]\nSaving walks for layer 49\n100%|█████████████████████████████████████| 160/160 [00:00<00:00, 385683.13it/s]\nSaving walks for layer 50\n100%|█████████████████████████████████████| 200/200 [00:00<00:00, 403298.46it/s]\nSaving walks for layer 51\n100%|█████████████████████████████████████| 220/220 [00:00<00:00, 405959.91it/s]\nSaving walks for layer 52\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 247451.56it/s]\nSaving walks for layer 53\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 388661.37it/s]\nSaving walks for layer 54\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 324720.31it/s]\nSaving walks for layer 55\n100%|█████████████████████████████████████| 140/140 [00:00<00:00, 329149.42it/s]\nSaving walks for layer 56\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 421891.43it/s]\nSaving walks for layer 57\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 334874.57it/s]\nSaving walks for layer 58\n100%|█████████████████████████████████████| 100/100 [00:00<00:00, 406819.01it/s]\nSaving walks for layer 59\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 317449.69it/s]\nSaving walks for layer 60\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 293051.81it/s]\nSaving walks for layer 61\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 315657.87it/s]\nSaving walks for layer 62\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 332551.36it/s]\nSaving walks for layer 63\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 327253.89it/s]\nSaving walks for layer 64\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 315065.09it/s]\nCounting vocab for layer 0\n100%|███████████████████████████████████| 7940/7940 [00:00<00:00, 688999.15it/s]\nCounting vocab for layer 1\n100%|███████████████████████████████████| 8980/8980 [00:00<00:00, 644030.74it/s]\nCounting vocab for layer 2\n100%|███████████████████████████████████| 9640/9640 [00:00<00:00, 646081.79it/s]\nCounting vocab for layer 3\n100%|███████████████████████████████████| 8440/8440 [00:00<00:00, 590569.65it/s]\nCounting vocab for layer 4\n100%|███████████████████████████████████| 6100/6100 [00:00<00:00, 622118.72it/s]\nCounting vocab for layer 5\n100%|███████████████████████████████████| 2220/2220 [00:00<00:00, 620674.24it/s]\nCounting vocab for layer 6\n100%|███████████████████████████████████| 1680/1680 [00:00<00:00, 599339.18it/s]\nCounting vocab for layer 7\n100%|███████████████████████████████████| 2000/2000 [00:00<00:00, 600860.11it/s]\nCounting vocab for layer 8\n100%|███████████████████████████████████| 5400/5400 [00:00<00:00, 639050.89it/s]\nCounting vocab for layer 9\n100%|███████████████████████████████████| 1600/1600 [00:00<00:00, 633461.05it/s]\nCounting vocab for layer 10\n100%|███████████████████████████████████| 1040/1040 [00:00<00:00, 634576.11it/s]\nCounting vocab for layer 11\n100%|███████████████████████████████████| 1620/1620 [00:00<00:00, 547347.55it/s]\nCounting vocab for layer 12\n100%|███████████████████████████████████| 2900/2900 [00:00<00:00, 619668.94it/s]\nCounting vocab for layer 13\n100%|█████████████████████████████████████| 740/740 [00:00<00:00, 594708.75it/s]\nCounting vocab for layer 14\n100%|█████████████████████████████████████| 980/980 [00:00<00:00, 651619.84it/s]\nCounting vocab for layer 15\n100%|███████████████████████████████████| 1020/1020 [00:00<00:00, 626473.87it/s]\nCounting vocab for layer 16\n100%|█████████████████████████████████████| 680/680 [00:00<00:00, 583615.04it/s]\nCounting vocab for layer 17\n100%|███████████████████████████████████| 1060/1060 [00:00<00:00, 646215.44it/s]\nCounting vocab for layer 18\n100%|███████████████████████████████████| 1340/1340 [00:00<00:00, 648404.17it/s]\nCounting vocab for layer 19\n100%|█████████████████████████████████████| 880/880 [00:00<00:00, 581166.35it/s]\nCounting vocab for layer 20\n100%|█████████████████████████████████████| 460/460 [00:00<00:00, 580264.61it/s]\nCounting vocab for layer 21\n100%|█████████████████████████████████████| 620/620 [00:00<00:00, 593578.74it/s]\nCounting vocab for layer 22\n100%|█████████████████████████████████████| 840/840 [00:00<00:00, 628696.53it/s]\nCounting vocab for layer 23\n100%|█████████████████████████████████████| 440/440 [00:00<00:00, 574204.65it/s]\nCounting vocab for layer 24\n100%|█████████████████████████████████████| 300/300 [00:00<00:00, 384093.77it/s]\nCounting vocab for layer 25\n100%|█████████████████████████████████████| 800/800 [00:00<00:00, 576933.15it/s]\nCounting vocab for layer 26\n100%|█████████████████████████████████████| 880/880 [00:00<00:00, 636596.67it/s]\nCounting vocab for layer 27\n100%|█████████████████████████████████████| 980/980 [00:00<00:00, 679969.88it/s]\nCounting vocab for layer 28\n100%|█████████████████████████████████████| 680/680 [00:00<00:00, 597053.95it/s]\nCounting vocab for layer 29\n100%|█████████████████████████████████████| 380/380 [00:00<00:00, 590090.90it/s]\nCounting vocab for layer 30\n100%|█████████████████████████████████████| 480/480 [00:00<00:00, 340942.58it/s]\nCounting vocab for layer 31\n100%|█████████████████████████████████████| 660/660 [00:00<00:00, 619709.12it/s]\nCounting vocab for layer 32\n100%|█████████████████████████████████████| 560/560 [00:00<00:00, 643333.40it/s]\nCounting vocab for layer 33\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 659284.31it/s]\nCounting vocab for layer 34\n100%|█████████████████████████████████████| 560/560 [00:00<00:00, 689002.71it/s]\nCounting vocab for layer 35\n100%|█████████████████████████████████████| 520/520 [00:00<00:00, 631270.07it/s]\nCounting vocab for layer 36\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 649081.68it/s]\nCounting vocab for layer 37\n100%|█████████████████████████████████████| 420/420 [00:00<00:00, 704079.81it/s]\nCounting vocab for layer 38\n100%|█████████████████████████████████████| 640/640 [00:00<00:00, 683563.68it/s]\nCounting vocab for layer 39\n100%|█████████████████████████████████████| 620/620 [00:00<00:00, 667814.20it/s]\nCounting vocab for layer 40\n100%|█████████████████████████████████████| 580/580 [00:00<00:00, 641533.84it/s]\nCounting vocab for layer 41\n100%|█████████████████████████████████████| 360/360 [00:00<00:00, 618324.91it/s]\nCounting vocab for layer 42\n100%|█████████████████████████████████████| 380/380 [00:00<00:00, 625769.74it/s]\nCounting vocab for layer 43\n100%|█████████████████████████████████████| 540/540 [00:00<00:00, 678121.01it/s]\nCounting vocab for layer 44\n100%|█████████████████████████████████████| 260/260 [00:00<00:00, 632918.77it/s]\nCounting vocab for layer 45\n100%|█████████████████████████████████████| 400/400 [00:00<00:00, 638402.44it/s]\nCounting vocab for layer 46\n100%|█████████████████████████████████████| 300/300 [00:00<00:00, 647935.74it/s]\nCounting vocab for layer 47\n100%|█████████████████████████████████████| 260/260 [00:00<00:00, 631452.83it/s]\nCounting vocab for layer 48\n100%|█████████████████████████████████████| 280/280 [00:00<00:00, 541450.03it/s]\nCounting vocab for layer 49\n100%|█████████████████████████████████████| 160/160 [00:00<00:00, 654082.50it/s]\nCounting vocab for layer 50\n100%|█████████████████████████████████████| 200/200 [00:00<00:00, 526922.61it/s]\nCounting vocab for layer 51\n100%|█████████████████████████████████████| 220/220 [00:00<00:00, 634626.46it/s]\nCounting vocab for layer 52\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 460912.53it/s]\nCounting vocab for layer 53\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 590054.49it/s]\nCounting vocab for layer 54\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 528693.78it/s]\nCounting vocab for layer 55\n100%|█████████████████████████████████████| 140/140 [00:00<00:00, 587790.35it/s]\nCounting vocab for layer 56\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 520492.74it/s]\nCounting vocab for layer 57\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 569362.53it/s]\nCounting vocab for layer 58\n100%|█████████████████████████████████████| 100/100 [00:00<00:00, 554802.12it/s]\nCounting vocab for layer 59\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 635500.61it/s]\nCounting vocab for layer 60\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 630722.41it/s]\nCounting vocab for layer 61\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 627185.64it/s]\nCounting vocab for layer 62\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 651542.37it/s]\nCounting vocab for layer 63\n100%|█████████████████████████████████████| 120/120 [00:00<00:00, 719023.54it/s]\nCounting vocab for layer 64\n100%|███████████████████████████████████████| 80/80 [00:00<00:00, 637916.96it/s]\nGenerating training pairs for layer 0\n100%|████████████████████████████████████| 7940/7940 [00:00<00:00, 64031.24it/s]\nGenerating training pairs for layer 1\n100%|████████████████████████████████████| 8980/8980 [00:00<00:00, 62272.62it/s]\nGenerating training pairs for layer 2\n100%|████████████████████████████████████| 9640/9640 [00:00<00:00, 63830.67it/s]\nGenerating training pairs for layer 3\n100%|████████████████████████████████████| 8440/8440 [00:00<00:00, 63853.03it/s]\nGenerating training pairs for layer 4\n100%|████████████████████████████████████| 6100/6100 [00:00<00:00, 63617.06it/s]\nGenerating training pairs for layer 5\n100%|████████████████████████████████████| 2220/2220 [00:00<00:00, 64053.68it/s]\nGenerating training pairs for layer 6\n100%|████████████████████████████████████| 1680/1680 [00:00<00:00, 62355.03it/s]\nGenerating training pairs for layer 7\n100%|████████████████████████████████████| 2000/2000 [00:00<00:00, 64091.44it/s]\nGenerating training pairs for layer 8\n100%|████████████████████████████████████| 5400/5400 [00:00<00:00, 62329.19it/s]\nGenerating training pairs for layer 9\n100%|████████████████████████████████████| 1600/1600 [00:00<00:00, 44542.96it/s]\nGenerating training pairs for layer 10\n100%|████████████████████████████████████| 1040/1040 [00:00<00:00, 58139.31it/s]\nGenerating training pairs for layer 11\n100%|████████████████████████████████████| 1620/1620 [00:00<00:00, 49546.97it/s]\nGenerating training pairs for layer 12\n100%|████████████████████████████████████| 2900/2900 [00:00<00:00, 61675.62it/s]\nGenerating training pairs for layer 13\n100%|██████████████████████████████████████| 740/740 [00:00<00:00, 62486.86it/s]\nGenerating training pairs for layer 14\n100%|██████████████████████████████████████| 980/980 [00:00<00:00, 61529.52it/s]\nGenerating training pairs for layer 15\n100%|████████████████████████████████████| 1020/1020 [00:00<00:00, 59919.47it/s]\nGenerating training pairs for layer 16\n100%|██████████████████████████████████████| 680/680 [00:00<00:00, 59586.89it/s]\nGenerating training pairs for layer 17\n100%|████████████████████████████████████| 1060/1060 [00:00<00:00, 62721.66it/s]\nGenerating training pairs for layer 18\n100%|████████████████████████████████████| 1340/1340 [00:00<00:00, 38415.15it/s]\nGenerating training pairs for layer 19\n100%|██████████████████████████████████████| 880/880 [00:00<00:00, 38692.03it/s]\nGenerating training pairs for layer 20\n100%|██████████████████████████████████████| 460/460 [00:00<00:00, 61077.59it/s]\nGenerating training pairs for layer 21\n100%|██████████████████████████████████████| 620/620 [00:00<00:00, 60559.10it/s]\nGenerating training pairs for layer 22\n100%|██████████████████████████████████████| 840/840 [00:00<00:00, 62620.47it/s]\nGenerating training pairs for layer 23\n100%|██████████████████████████████████████| 440/440 [00:00<00:00, 62209.05it/s]\nGenerating training pairs for layer 24\n100%|██████████████████████████████████████| 300/300 [00:00<00:00, 63351.69it/s]\nGenerating training pairs for layer 25\n100%|██████████████████████████████████████| 800/800 [00:00<00:00, 62863.09it/s]\nGenerating training pairs for layer 26\n100%|██████████████████████████████████████| 880/880 [00:00<00:00, 58955.81it/s]\nGenerating training pairs for layer 27\n100%|██████████████████████████████████████| 980/980 [00:00<00:00, 62368.83it/s]\nGenerating training pairs for layer 28\n100%|██████████████████████████████████████| 680/680 [00:00<00:00, 59086.94it/s]\nGenerating training pairs for layer 29\n100%|██████████████████████████████████████| 380/380 [00:00<00:00, 63960.65it/s]\nGenerating training pairs for layer 30\n100%|██████████████████████████████████████| 480/480 [00:00<00:00, 48183.85it/s]\nGenerating training pairs for layer 31\n100%|██████████████████████████████████████| 660/660 [00:00<00:00, 49539.91it/s]\nGenerating training pairs for layer 32\n100%|██████████████████████████████████████| 560/560 [00:00<00:00, 62548.21it/s]\nGenerating training pairs for layer 33\n100%|██████████████████████████████████████| 420/420 [00:00<00:00, 61010.17it/s]\nGenerating training pairs for layer 34\n100%|██████████████████████████████████████| 560/560 [00:00<00:00, 46305.70it/s]\nGenerating training pairs for layer 35\n100%|██████████████████████████████████████| 520/520 [00:00<00:00, 38807.13it/s]\nGenerating training pairs for layer 36\n100%|██████████████████████████████████████| 420/420 [00:00<00:00, 26948.67it/s]\nGenerating training pairs for layer 37\n100%|██████████████████████████████████████| 420/420 [00:00<00:00, 32586.16it/s]\nGenerating training pairs for layer 38\n100%|██████████████████████████████████████| 640/640 [00:00<00:00, 54263.37it/s]\nGenerating training pairs for layer 39\n100%|██████████████████████████████████████| 620/620 [00:00<00:00, 53422.94it/s]\nGenerating training pairs for layer 40\n100%|██████████████████████████████████████| 580/580 [00:00<00:00, 52914.61it/s]\nGenerating training pairs for layer 41\n100%|██████████████████████████████████████| 360/360 [00:00<00:00, 54211.38it/s]\nGenerating training pairs for layer 42\n100%|██████████████████████████████████████| 380/380 [00:00<00:00, 51806.78it/s]\nGenerating training pairs for layer 43\n100%|██████████████████████████████████████| 540/540 [00:00<00:00, 52451.87it/s]\nGenerating training pairs for layer 44\n100%|██████████████████████████████████████| 260/260 [00:00<00:00, 50956.45it/s]\nGenerating training pairs for layer 45\n100%|██████████████████████████████████████| 400/400 [00:00<00:00, 44503.08it/s]\nGenerating training pairs for layer 46\n100%|██████████████████████████████████████| 300/300 [00:00<00:00, 51202.08it/s]\nGenerating training pairs for layer 47\n100%|██████████████████████████████████████| 260/260 [00:00<00:00, 51993.85it/s]\nGenerating training pairs for layer 48\n100%|██████████████████████████████████████| 280/280 [00:00<00:00, 53331.14it/s]\nGenerating training pairs for layer 49\n100%|██████████████████████████████████████| 160/160 [00:00<00:00, 49950.77it/s]\nGenerating training pairs for layer 50\n100%|██████████████████████████████████████| 200/200 [00:00<00:00, 51013.18it/s]\nGenerating training pairs for layer 51\n100%|██████████████████████████████████████| 220/220 [00:00<00:00, 52788.72it/s]\nGenerating training pairs for layer 52\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 45462.60it/s]\nGenerating training pairs for layer 53\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 50081.24it/s]\nGenerating training pairs for layer 54\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 51150.05it/s]\nGenerating training pairs for layer 55\n100%|██████████████████████████████████████| 140/140 [00:00<00:00, 51194.64it/s]\nGenerating training pairs for layer 56\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 50968.76it/s]\nGenerating training pairs for layer 57\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 34797.88it/s]\nGenerating training pairs for layer 58\n100%|██████████████████████████████████████| 100/100 [00:00<00:00, 53821.43it/s]\nGenerating training pairs for layer 59\n100%|████████████████████████████████████████| 80/80 [00:00<00:00, 48321.47it/s]\nGenerating training pairs for layer 60\n100%|████████████████████████████████████████| 80/80 [00:00<00:00, 49504.92it/s]\nGenerating training pairs for layer 61\n100%|████████████████████████████████████████| 80/80 [00:00<00:00, 48342.36it/s]\nGenerating training pairs for layer 62\n100%|████████████████████████████████████████| 80/80 [00:00<00:00, 49999.15it/s]\nGenerating training pairs for layer 63\n100%|██████████████████████████████████████| 120/120 [00:00<00:00, 53572.80it/s]\nGenerating training pairs for layer 64\n100%|████████████████████████████████████████| 80/80 [00:00<00:00, 52535.51it/s]\nGenerating neighbors for layer 0\n100%|██████████████████████████████████| 6376/6376 [00:00<00:00, 1454840.73it/s]\nGenerating neighbors for layer 1\n100%|██████████████████████████████████| 6176/6176 [00:00<00:00, 1492683.04it/s]\nGenerating neighbors for layer 2\n100%|██████████████████████████████████| 3669/3669 [00:00<00:00, 1404096.84it/s]\nGenerating neighbors for layer 3\n100%|██████████████████████████████████| 1550/1550 [00:00<00:00, 1193094.37it/s]\nGenerating neighbors for layer 4\n100%|████████████████████████████████████| 853/853 [00:00<00:00, 1116996.98it/s]\nGenerating neighbors for layer 5\n100%|████████████████████████████████████| 735/735 [00:00<00:00, 1140515.52it/s]\nGenerating neighbors for layer 6\n100%|████████████████████████████████████| 716/716 [00:00<00:00, 1168075.33it/s]\nGenerating neighbors for layer 7\n100%|████████████████████████████████████| 705/705 [00:00<00:00, 1159601.69it/s]\nGenerating neighbors for layer 8\n100%|█████████████████████████████████████| 451/451 [00:00<00:00, 936451.04it/s]\nGenerating neighbors for layer 9\n100%|█████████████████████████████████████| 358/358 [00:00<00:00, 883790.95it/s]\nGenerating neighbors for layer 10\n100%|████████████████████████████████████| 235/235 [00:00<00:00, 1117529.98it/s]\nGenerating neighbors for layer 11\n100%|████████████████████████████████████| 206/206 [00:00<00:00, 1016501.91it/s]\nGenerating neighbors for layer 12\n100%|█████████████████████████████████████| 159/159 [00:00<00:00, 738531.93it/s]\nGenerating neighbors for layer 13\n100%|█████████████████████████████████████| 159/159 [00:00<00:00, 990927.69it/s]\nGenerating neighbors for layer 14\n100%|█████████████████████████████████████| 122/122 [00:00<00:00, 858565.58it/s]\nGenerating neighbors for layer 15\n100%|█████████████████████████████████████| 107/107 [00:00<00:00, 814501.87it/s]\nGenerating neighbors for layer 16\n100%|█████████████████████████████████████| 105/105 [00:00<00:00, 842068.68it/s]\nGenerating neighbors for layer 17\n100%|█████████████████████████████████████| 103/103 [00:00<00:00, 815119.46it/s]\nGenerating neighbors for layer 18\n100%|█████████████████████████████████████| 102/102 [00:00<00:00, 745329.28it/s]\nGenerating neighbors for layer 19\n100%|█████████████████████████████████████| 100/100 [00:00<00:00, 800439.69it/s]\nGenerating neighbors for layer 20\n100%|███████████████████████████████████████| 81/81 [00:00<00:00, 671420.21it/s]\nGenerating neighbors for layer 21\n100%|███████████████████████████████████████| 66/66 [00:00<00:00, 633464.68it/s]\nGenerating neighbors for layer 22\n100%|███████████████████████████████████████| 65/65 [00:00<00:00, 664950.63it/s]\nGenerating neighbors for layer 23\n100%|███████████████████████████████████████| 61/61 [00:00<00:00, 656032.16it/s]\nGenerating neighbors for layer 24\n100%|███████████████████████████████████████| 59/59 [00:00<00:00, 519882.22it/s]\nGenerating neighbors for layer 25\n100%|███████████████████████████████████████| 52/52 [00:00<00:00, 442401.23it/s]\nGenerating neighbors for layer 26\n100%|███████████████████████████████████████| 50/50 [00:00<00:00, 554802.12it/s]\nGenerating neighbors for layer 27\n100%|███████████████████████████████████████| 48/48 [00:00<00:00, 462819.75it/s]\nGenerating neighbors for layer 28\n100%|███████████████████████████████████████| 45/45 [00:00<00:00, 510118.05it/s]\nGenerating neighbors for layer 29\n100%|███████████████████████████████████████| 43/43 [00:00<00:00, 601183.57it/s]\nGenerating neighbors for layer 30\n100%|███████████████████████████████████████| 41/41 [00:00<00:00, 625332.60it/s]\nGenerating neighbors for layer 31\n100%|███████████████████████████████████████| 40/40 [00:00<00:00, 584571.99it/s]\nGenerating neighbors for layer 32\n100%|███████████████████████████████████████| 38/38 [00:00<00:00, 664098.13it/s]\nGenerating neighbors for layer 33\n100%|███████████████████████████████████████| 37/37 [00:00<00:00, 352702.84it/s]\nGenerating neighbors for layer 34\n100%|███████████████████████████████████████| 37/37 [00:00<00:00, 444668.33it/s]\nGenerating neighbors for layer 35\n100%|███████████████████████████████████████| 35/35 [00:00<00:00, 587202.56it/s]\nGenerating neighbors for layer 36\n100%|███████████████████████████████████████| 35/35 [00:00<00:00, 635500.61it/s]\nGenerating neighbors for layer 37\n100%|███████████████████████████████████████| 33/33 [00:00<00:00, 586491.66it/s]\nGenerating neighbors for layer 38\n100%|███████████████████████████████████████| 31/31 [00:00<00:00, 585691.10it/s]\nGenerating neighbors for layer 39\n100%|███████████████████████████████████████| 31/31 [00:00<00:00, 347656.21it/s]\nGenerating neighbors for layer 40\n100%|███████████████████████████████████████| 28/28 [00:00<00:00, 584281.15it/s]\nGenerating neighbors for layer 41\n100%|███████████████████████████████████████| 28/28 [00:00<00:00, 618107.96it/s]\nGenerating neighbors for layer 42\n100%|███████████████████████████████████████| 26/26 [00:00<00:00, 486838.86it/s]\nGenerating neighbors for layer 43\n100%|███████████████████████████████████████| 26/26 [00:00<00:00, 521779.44it/s]\nGenerating neighbors for layer 44\n100%|███████████████████████████████████████| 26/26 [00:00<00:00, 573957.39it/s]\nGenerating neighbors for layer 45\n100%|███████████████████████████████████████| 22/22 [00:00<00:00, 419430.40it/s]\nGenerating neighbors for layer 46\n100%|███████████████████████████████████████| 22/22 [00:00<00:00, 509804.91it/s]\nGenerating neighbors for layer 47\n100%|███████████████████████████████████████| 13/13 [00:00<00:00, 370924.84it/s]\nGenerating neighbors for layer 48\n100%|███████████████████████████████████████| 13/13 [00:00<00:00, 358723.37it/s]\nGenerating neighbors for layer 49\n100%|█████████████████████████████████████████| 9/9 [00:00<00:00, 288158.29it/s]\nGenerating neighbors for layer 50\n100%|█████████████████████████████████████████| 8/8 [00:00<00:00, 244922.86it/s]\nGenerating neighbors for layer 51\n100%|█████████████████████████████████████████| 7/7 [00:00<00:00, 224123.11it/s]\nGenerating neighbors for layer 52\n100%|█████████████████████████████████████████| 6/6 [00:00<00:00, 202950.19it/s]\nGenerating neighbors for layer 53\n100%|█████████████████████████████████████████| 6/6 [00:00<00:00, 206277.25it/s]\nGenerating neighbors for layer 54\n100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 158875.15it/s]\nGenerating neighbors for layer 55\n100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 131896.35it/s]\nGenerating neighbors for layer 56\n100%|█████████████████████████████████████████| 5/5 [00:00<00:00, 113359.57it/s]\nGenerating neighbors for layer 57\n100%|█████████████████████████████████████████| 4/4 [00:00<00:00, 113359.57it/s]\nGenerating neighbors for layer 58\n100%|█████████████████████████████████████████| 4/4 [00:00<00:00, 147168.56it/s]\nGenerating neighbors for layer 59\n100%|█████████████████████████████████████████| 3/3 [00:00<00:00, 113359.57it/s]\nGenerating neighbors for layer 60\n100%|█████████████████████████████████████████| 3/3 [00:00<00:00, 111353.20it/s]\nGenerating neighbors for layer 61\n100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 73584.28it/s]\nGenerating neighbors for layer 62\n100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 20832.64it/s]\nGenerating neighbors for layer 63\n100%|██████████████████████████████████████████| 3/3 [00:00<00:00, 95325.09it/s]\nGenerating neighbors for layer 64\n100%|█████████████████████████████████████████| 3/3 [00:00<00:00, 110376.42it/s]\nfeature dimension: 572\nOptimizing\nTraining\n{'epoch': 0, 'iter': 0, 'avg_loss': 23.088226318359375, 'loss': 23.088226}\n{'epoch': 0, 'iter': 5000, 'avg_loss': 11.267947198783986, 'loss': 7.338138}\n{'epoch': 0, 'iter': 10000, 'avg_loss': 8.847510078944822, 'loss': 6.90406}\n{'epoch': 0, 'iter': 15000, 'avg_loss': 7.370908860881824, 'loss': 5.8131475}\n{'epoch': 0, 'iter': 20000, 'avg_loss': 6.391495063748218, 'loss': 2.698447}\n{'epoch': 0, 'iter': 25000, 'avg_loss': 5.682726601014962, 'loss': 1.6807973}\n{'epoch': 0, 'iter': 30000, 'avg_loss': 5.154749970583752, 'loss': 2.0424151}\n{'epoch': 0, 'iter': 35000, 'avg_loss': 4.745977672719025, 'loss': 2.5076892}\n{'epoch': 0, 'iter': 40000, 'avg_loss': 4.422325140154215, 'loss': 2.0127726}\nepoch 0: 100%|| 44222/44222 [08:43<00:00, 84.49it/s]\nvalid auc: 0.72450203421734\nvalid pr: 0.7393766856566806\nvalid f1: 0.6910622472203675\nOverall ROC-AUC: 0.6824512308609979\nOverall PR-AUC 0.6978684282340724\nOverall F1: 0.6522351026444138\n","output_type":"stream"}],"execution_count":5},{"id":"616c853c","cell_type":"code","source":"!git pull origin main","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:55:34.804491Z","iopub.execute_input":"2024-12-03T19:55:34.804819Z","iopub.status.idle":"2024-12-03T19:55:35.964879Z","shell.execute_reply.started":"2024-12-03T19:55:34.80479Z","shell.execute_reply":"2024-12-03T19:55:35.963817Z"},"papermill":{"duration":1.448581,"end_time":"2024-12-02T23:19:44.20124","exception":false,"start_time":"2024-12-02T23:19:42.752659","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"From https://github.com/OmarKhalifaa/GNN_DDI\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":6},{"id":"dd63dac3","cell_type":"code","source":"!python concat.py\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:55:35.966493Z","iopub.execute_input":"2024-12-03T19:55:35.966911Z","iopub.status.idle":"2024-12-03T19:55:47.822912Z","shell.execute_reply.started":"2024-12-03T19:55:35.966869Z","shell.execute_reply":"2024-12-03T19:55:47.822036Z"},"papermill":{"duration":12.25547,"end_time":"2024-12-02T23:19:56.721328","exception":false,"start_time":"2024-12-02T23:19:44.465858","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/working/GNN_DDI/DDI/data5/final_modelss1.csv shape after processing: (37050, 34)\n/kaggle/working/GNN_DDI/DDI/data5/final_modelss2.csv shape after processing: (37050, 34)\n/kaggle/working/GNN_DDI/DDI/data5/final_modelss3.csv shape after processing: (37050, 34)\n/kaggle/working/GNN_DDI/DDI/data5/final_modelss4.csv shape after processing: (37050, 34)\nReduced data 1 length: 570\nReduced data 2 length: 570\nReduced data 3 length: 570\nReduced data 4 length: 570\nLength of xs1: 570\nNumber of feature dictionaries: 4\nFirst 5 keys in xs1: [0, 1, 2, 3, 4]\nfull_pos shape: (37264, 3)\nDDI shape: (37264, 2)\nMultiplying indices 541 and 351: [0.02656227 0.02790939 0.02288867 0.02503109 0.02487968 0.02385757\n 0.02426968 0.03031032 0.02626601 0.02548944 0.02430399 0.03041907\n 0.0210613  0.01759591 0.02716357 0.0269257  0.02123744 0.02493482\n 0.02905957 0.02325038 0.02991576 0.0243281  0.02095308 0.02011403\n 0.0281166  0.02438699 0.01772582 0.02723098 0.02379801 0.02060952\n 0.02583191 0.02116332]\nMultiplying indices 541 and 286: [0.0298486  0.02505496 0.02124569 0.02160578 0.02291188 0.02725282\n 0.02622872 0.02774258 0.02707007 0.02623951 0.02454978 0.02641176\n 0.02176168 0.016706   0.02468058 0.02699371 0.02031457 0.02495294\n 0.03187592 0.02448431 0.02817712 0.02257268 0.02038281 0.01798635\n 0.0299232  0.03188361 0.02070294 0.02778583 0.02144929 0.02252942\n 0.02807871 0.02343973]\nMultiplying indices 541 and 149: [0.0332038  0.02896088 0.02354496 0.02570142 0.03305005 0.02074411\n 0.02348606 0.02616058 0.0269374  0.03024961 0.02618999 0.03990289\n 0.02332601 0.0199756  0.02755325 0.01983554 0.02193956 0.02666219\n 0.02528155 0.01814226 0.03036592 0.01476323 0.02239094 0.01975857\n 0.03365525 0.01514438 0.02047737 0.02946364 0.02203275 0.01786932\n 0.02201791 0.0196534 ]\nMultiplying indices 541 and 280: [0.02419196 0.02667836 0.02308935 0.02365825 0.02475614 0.02570044\n 0.02389838 0.03055843 0.02649645 0.02646495 0.02266759 0.02919164\n 0.02073075 0.01588236 0.02633476 0.02851516 0.0239366  0.02158712\n 0.02819199 0.02335974 0.03338615 0.02561495 0.02079892 0.01774789\n 0.02659083 0.02854038 0.01566455 0.02720337 0.02413551 0.02315125\n 0.02837493 0.02310313]\nMultiplying indices 541 and 43: [0.02758421 0.02783309 0.02201351 0.02477616 0.02393216 0.02655521\n 0.02317812 0.03024953 0.02745185 0.02568114 0.02361489 0.02703386\n 0.02105369 0.0179015  0.02800685 0.02567607 0.02191914 0.02273181\n 0.02954613 0.02387679 0.02976817 0.02355174 0.02092724 0.01902483\n 0.02844194 0.0276934  0.0203965  0.02597411 0.02273247 0.02158253\n 0.02796099 0.02159565]\nMultiplying indices 541 and 351: [0.01316689 0.01668669 0.01663512 0.03134639 0.03160077 0.01512133\n 0.02704465 0.00460006 0.01359443 0.02842583 0.00680545 0.01740241\n 0.01968204 0.02976654 0.01311007 0.02014731 0.04280402 0.01649851\n 0.01982278 0.01792592 0.00526959 0.02006379 0.01746405 0.0131613\n 0.02311382 0.02171059 0.02886219 0.0169537  0.02724764 0.01520395\n 0.03573769 0.0195005 ]\nMultiplying indices 541 and 286: [0.01320031 0.01632574 0.01519586 0.03203774 0.03270405 0.01458473\n 0.02475905 0.00761368 0.01381238 0.02513058 0.00795987 0.01755921\n 0.01721826 0.02851976 0.01736302 0.01977807 0.03608815 0.01403248\n 0.02551701 0.01559846 0.00478074 0.0275172  0.01419013 0.01989709\n 0.01514783 0.01886086 0.0143143  0.02370042 0.02841098 0.02383904\n 0.0244606  0.01859596]\nMultiplying indices 541 and 149: [0.01404355 0.02708598 0.00841477 0.01930063 0.04148506 0.0113579\n 0.02546228 0.00548354 0.00969982 0.023805   0.00443668 0.02104166\n 0.02896847 0.03382041 0.01109994 0.01566485 0.05315779 0.01926629\n 0.03165952 0.01028032 0.00284968 0.01677523 0.01383872 0.01566871\n 0.0228535  0.02561648 0.03043357 0.02670461 0.0341008  0.02342521\n 0.04113243 0.01859852]\nMultiplying indices 541 and 280: [0.01478571 0.01960068 0.0146465  0.02738729 0.03147595 0.01637646\n 0.02613558 0.00584221 0.01472161 0.02347025 0.00676857 0.01868729\n 0.0200228  0.03168362 0.01470177 0.01843572 0.04339745 0.01374619\n 0.01732981 0.01534435 0.00503731 0.02267224 0.01553814 0.01736174\n 0.01900107 0.02485912 0.03259467 0.01908293 0.03070026 0.01298917\n 0.03299975 0.01936498]\nMultiplying indices 541 and 43: [0.01187124 0.02442231 0.01276064 0.02525123 0.03151636 0.012544\n 0.02468168 0.00478981 0.01262632 0.01959598 0.00583359 0.01688823\n 0.02036824 0.03225674 0.01469065 0.01905493 0.04174834 0.01075308\n 0.02413042 0.01080706 0.00418839 0.02387539 0.01403654 0.01107958\n 0.0211215  0.02231758 0.02927641 0.01876833 0.02768415 0.01987686\n 0.0297249  0.0162057 ]\nMultiplying indices 541 and 351: [0.02352172 0.02604064 0.02306413 0.02162985 0.01917259 0.01739959\n 0.02148867 0.02459588 0.02380731 0.03017438 0.02234912 0.0224165\n 0.0279382  0.02363752 0.02864422 0.02716876 0.02397394 0.02223823\n 0.02969931 0.01971161 0.03164308 0.02264943 0.03303849 0.0265988\n 0.02635328 0.02719135 0.0266888  0.01773934 0.02357285 0.02558184\n 0.02617062 0.02197901]\nMultiplying indices 541 and 286: [0.0238871  0.02706694 0.02329515 0.02100249 0.01949708 0.01967815\n 0.02167841 0.02490029 0.02606212 0.0297152  0.02035182 0.02304787\n 0.02547906 0.02194858 0.02975427 0.02761256 0.02336126 0.02212876\n 0.03333779 0.02278664 0.02755967 0.02187515 0.02838341 0.02493581\n 0.02392024 0.0268105  0.02981305 0.01875323 0.02404567 0.02988824\n 0.02494666 0.0236545 ]\nMultiplying indices 541 and 149: [0.02135354 0.02834082 0.02244159 0.01898171 0.02247598 0.02016251\n 0.02467137 0.01741954 0.02344473 0.02928703 0.01990291 0.01785523\n 0.0250247  0.02429988 0.0310703  0.02551901 0.0261708  0.02630649\n 0.03072802 0.02041907 0.03032889 0.02382768 0.03049122 0.02683704\n 0.02713094 0.02787527 0.03087932 0.01847923 0.02272072 0.02537925\n 0.02537806 0.02322928]\nMultiplying indices 541 and 280: [0.02597921 0.02758174 0.02433889 0.02135242 0.01889884 0.01827829\n 0.02029006 0.02625296 0.02506635 0.02949619 0.02038642 0.02254392\n 0.02709017 0.02433763 0.03010285 0.02532853 0.02467289 0.02050394\n 0.02974651 0.01909964 0.03040853 0.02312853 0.03428466 0.02524878\n 0.0263045  0.02453525 0.02574545 0.01720588 0.02300891 0.02718929\n 0.02527499 0.02134372]\nMultiplying indices 541 and 43: [0.0254064  0.02524703 0.02555433 0.02066205 0.01979849 0.01805406\n 0.01964753 0.02641337 0.02519187 0.0293951  0.02299049 0.02074571\n 0.02613585 0.02512604 0.03041677 0.02583323 0.02190951 0.01954013\n 0.02961772 0.01731456 0.03153052 0.0232197  0.03276174 0.02706047\n 0.02787473 0.02545553 0.02468661 0.0169776  0.02666539 0.03008776\n 0.02542222 0.01995136]\nMultiplying indices 541 and 351: [0.02559155 0.0215708  0.0236023  0.01450176 0.02366413 0.01896083\n 0.0202501  0.02143772 0.01873448 0.01179346 0.02198467 0.02669458\n 0.03642545 0.03314167 0.02327229 0.01846294 0.01557127 0.01522092\n 0.03533079 0.0251898  0.02181486 0.02204972 0.02248546 0.03110865\n 0.01504386 0.01820028 0.01340734 0.02557416 0.0221576  0.02283935\n 0.01222897 0.03151391]\nMultiplying indices 541 and 286: [0.02080091 0.02282855 0.01712184 0.01285107 0.02447934 0.0191514\n 0.02796399 0.01751231 0.01822624 0.01335594 0.02120649 0.02263643\n 0.03286477 0.02840386 0.01915278 0.02340809 0.01949689 0.02736709\n 0.02784478 0.02540837 0.01792418 0.02253375 0.02061477 0.02314944\n 0.01607297 0.02154675 0.01817315 0.02349968 0.02011229 0.01942602\n 0.02189219 0.02956827]\nMultiplying indices 541 and 149: [0.02434547 0.02721503 0.02129434 0.01456214 0.02296421 0.0226863\n 0.02168987 0.01661385 0.02462966 0.01525774 0.02337159 0.02926\n 0.0375     0.03348794 0.02717707 0.01840838 0.01037314 0.02136081\n 0.03465556 0.03158252 0.02722102 0.01412281 0.02177786 0.03567225\n 0.01269404 0.01558247 0.01205403 0.02442768 0.02409741 0.01842605\n 0.01987329 0.02267691]\nMultiplying indices 541 and 280: [0.02342584 0.02114053 0.01860392 0.01178135 0.02464585 0.01814776\n 0.020836   0.02334332 0.02081381 0.01353683 0.02280624 0.02185091\n 0.03164467 0.02645226 0.02942009 0.01980739 0.0162871  0.020978\n 0.02662489 0.02677222 0.02348369 0.02431045 0.02299339 0.03163283\n 0.01449662 0.0176347  0.01337481 0.02509134 0.01747865 0.02136388\n 0.01685216 0.02424517]\nMultiplying indices 541 and 43: [0.01987333 0.01877806 0.02404355 0.01552509 0.02874403 0.01940602\n 0.02442901 0.01749294 0.02329076 0.01564647 0.02248937 0.02178738\n 0.03046645 0.02336622 0.02633951 0.01996081 0.0161217  0.0195575\n 0.03466501 0.02593987 0.02523388 0.02001286 0.01918775 0.03749618\n 0.01504088 0.01586047 0.0133081  0.02493977 0.01752009 0.02350206\n 0.01228272 0.02548849]\nSaved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt with shape (37264, 32)\nSaved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt with shape (37264, 32)\nSaved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt with shape (37264, 32)\nSaved multiplied features to /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt with shape (37264, 32)\nCleanup completed. Large variables have been deleted.\n","output_type":"stream"}],"execution_count":7},{"id":"47a7ee2d","cell_type":"code","source":"!git pull origin main","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:55:47.8242Z","iopub.execute_input":"2024-12-03T19:55:47.82449Z","iopub.status.idle":"2024-12-03T19:55:49.041509Z","shell.execute_reply.started":"2024-12-03T19:55:47.824461Z","shell.execute_reply":"2024-12-03T19:55:49.040688Z"},"papermill":{"duration":1.428123,"end_time":"2024-12-02T23:19:58.367144","exception":false,"start_time":"2024-12-02T23:19:56.939021","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"From https://github.com/OmarKhalifaa/GNN_DDI\n * branch            main       -> FETCH_HEAD\nAlready up to date.\n","output_type":"stream"}],"execution_count":8},{"id":"810be7c6-a901-4e34-97ec-faaab300662d","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\n# Load test data\ntest_data = pd.read_csv('/kaggle/working/GNN_DDI/DDI/data5/test.txt', sep=' ', header=None)\nprint(\"Raw test data shape:\", test_data.shape)\n\n# Extract features (assume first column is ID or label, exclude it)\nX_test_raw = test_data.iloc[:, 1:].values\n\n# Ensure it matches the model's input shape (128 features)\nif X_test_raw.shape[1] < 128:\n    X_test = np.pad(X_test_raw, ((0, 0), (0, 128 - X_test_raw.shape[1])), mode='constant')\nelif X_test_raw.shape[1] > 128:\n    X_test = X_test_raw[:, :128]\nelse:\n    X_test = X_test_raw\n\nprint(\"Adjusted test data shape:\", X_test.shape)\n\n# Load the trained model\nmodel = load_model('/kaggle/working/GNN_DDI/DDI/model_trained_final.h5')\n\n# Test the model\npredictions = model.predict(X_test)\nprint(\"Shape of predictions:\", predictions.shape)\n\n# Output predictions (e.g., for inspection or further processing)\nprint(\"Predictions:\", predictions.flatten())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"91680313","cell_type":"code","source":"!python train.py\n","metadata":{"execution":{"iopub.status.busy":"2024-12-03T19:55:49.042839Z","iopub.execute_input":"2024-12-03T19:55:49.043117Z","iopub.status.idle":"2024-12-03T20:06:05.882081Z","shell.execute_reply.started":"2024-12-03T19:55:49.043088Z","shell.execute_reply":"2024-12-03T20:06:05.881129Z"},"papermill":{"duration":681.501461,"end_time":"2024-12-02T23:31:20.091331","exception":false,"start_time":"2024-12-02T23:19:58.58987","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Loaded full_pos2.txt with shape: (37264, 3)\nnew_label: 37264, first label: 1\nCross-Validation Fold 1/5\nProcessing Feature 1/4: 1\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733255756.424226    1534 service.cc:145] XLA service 0x7bcbac00ac30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733255756.424285    1534 service.cc:153]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1733255756.424290    1534 service.cc:153]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1733255759.826507    1534 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nProcessing Feature 2/4: 2\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nProcessing Feature 3/4: 3\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nProcessing Feature 4/4: 4\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\nCross-Validation Fold 2/5\nProcessing Feature 1/4: 1\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nProcessing Feature 2/4: 2\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nProcessing Feature 3/4: 3\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nProcessing Feature 4/4: 4\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\nCross-Validation Fold 3/5\nProcessing Feature 1/4: 1\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nProcessing Feature 2/4: 2\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nProcessing Feature 3/4: 3\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nProcessing Feature 4/4: 4\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\nCross-Validation Fold 4/5\nProcessing Feature 1/4: 1\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nProcessing Feature 2/4: 2\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nProcessing Feature 3/4: 3\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nProcessing Feature 4/4: 4\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\nCross-Validation Fold 5/5\nProcessing Feature 1/4: 1\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nProcessing Feature 2/4: 2\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nProcessing Feature 3/4: 3\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nProcessing Feature 4/4: 4\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/kaggle/working/GNN_DDI/train.py:288: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  result_eve[i, 1] = roc_aupr_score(y_true_event, pred_score[:, i], average=None)\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nOverall Evaluation Results:\n [[0.76902641]\n [0.82912303]\n [0.77986273]\n [0.99554212]\n [0.982766  ]\n [0.76902641]\n [0.61253961]\n [0.76902641]\n [0.83760349]\n [0.76902641]\n [0.53363645]]\nPer-Event Evaluation Results:\n [[0.8637559  0.85468415 0.94753096 0.77938556 0.67923957 0.91416922]\n [0.90454594 0.90023952 0.95721423 0.82063436 0.78732463 0.85688711]\n [0.93009339 0.80834027 0.94058127 0.74760198 0.82524064 0.68331562]\n [0.97845105 0.88036151 0.98129174 0.82734896 0.84944812 0.80637049]\n [0.98553564 0.83630181 0.98195911 0.78008976 0.83933275 0.72865854]\n [0.9839255  0.84209364 0.99208719 0.66887783 0.89364845 0.5344523 ]\n [0.97845105 0.70127449 0.97989863 0.48558616 0.82570806 0.34392015]\n [0.98827286 0.85977006 0.98919556 0.77783427 0.86833144 0.70441989]\n [0.9878435  0.72898073 0.99245046 0.68210526 0.66575342 0.69928058]\n [0.99420352 0.90131166 0.99675498 0.76824034 0.93963255 0.64972777]\n [0.99589416 0.880112   0.99673684 0.74285714 0.94849785 0.61049724]\n [0.9971286  0.87594056 0.99774616 0.82658023 0.85284281 0.80188679]\n [0.99490125 0.56781942 0.96343279 0.48087432 0.72727273 0.35918367]\n [0.99677973 0.83591125 0.99736462 0.71014493 0.86982249 0.6       ]\n [0.99616252 0.66334308 0.99321434 0.43027888 0.85714286 0.28723404]\n [0.9983362  0.87769466 0.99126922 0.77205882 0.98130841 0.63636364]\n [0.99876556 0.96512226 0.99970153 0.83802817 0.98347107 0.73006135]\n [0.99667239 0.62212662 0.99613353 0.39215686 0.88888889 0.25157233]\n [0.99863139 0.92573297 0.99968035 0.84866469 0.79888268 0.90506329]\n [0.9996243  0.98769587 0.99993263 0.95394737 0.96666667 0.94155844]\n [0.99857772 0.83933695 0.99654998 0.75799087 0.89247312 0.65873016]\n [0.99847037 0.79801139 0.99923441 0.70157068 0.75280899 0.65686275]\n [0.99847037 0.78793881 0.99864493 0.65030675 0.84126984 0.53      ]\n [0.99801417 0.77849801 0.9991824  0.38333333 0.92       0.24210526]\n [0.99761164 0.57112686 0.99776655 0.06315789 1.         0.0326087 ]\n [0.9991681  0.78050583 0.99883957 0.78911565 0.87878788 0.71604938]\n [0.9984167  0.63627316 0.97937137 0.39175258 0.95       0.24675325]\n [0.99873873 0.70516808 0.99896134 0.56880734 0.91176471 0.41333333]\n [0.99895341 0.73340914 0.99637653 0.62135922 0.96969697 0.45714286]\n [0.99887291 0.81879414 0.99946232 0.60377358 0.82051282 0.47761194]\n [0.99930228 0.86067269 0.99966734 0.75925926 0.93181818 0.640625  ]\n [0.99881924 0.78458449 0.99968351 0.52173913 0.8        0.38709677]\n [0.9984167  0.45780893 0.99593184 0.19178082 0.5        0.11864407]\n [0.9987924  0.80855949 0.99964967 0.4        0.88235294 0.25862069]\n [0.99978532 0.96500506 0.99990805 0.92857143 0.94545455 0.9122807 ]\n [0.99932911 0.86724914 0.99977327 0.76190476 0.8        0.72727273]\n [0.99975848 0.96666481 0.99993879 0.90909091 1.         0.83333333]\n [0.99900708 0.77863674 0.99441161 0.44776119 0.9375     0.29411765]\n [0.99863139 0.15890652 0.9726745  0.         0.         0.        ]\n [0.99943645 0.84472578 0.99817451 0.77894737 0.78723404 0.77083333]\n [0.99986582 0.97805818 0.99994993 0.93975904 1.         0.88636364]\n [0.99951696 0.89045175 0.99985834 0.79545455 0.79545455 0.79545455]\n [0.99930228 0.65462868 0.99906982 0.53571429 0.9375     0.375     ]\n [0.9991681  0.78724225 0.99537059 0.39215686 0.90909091 0.25      ]\n [0.99946329 0.94348329 0.99990866 0.66666667 1.         0.5       ]\n [0.9991681  0.5021944  0.99758259 0.34042553 0.61538462 0.23529412]\n [0.99957063 0.88538158 0.99956945 0.7037037  0.95       0.55882353]\n [0.99957063 0.68899139 0.93412632 0.38461538 1.         0.23809524]\n [0.99986582 0.92359055 0.99991005 0.85714286 1.         0.75      ]\n [0.99967797 0.74237315 0.99696278 0.33333333 1.         0.2       ]\n [0.99994633 0.88380543 0.92412181 0.91666667 1.         0.84615385]\n [0.99970481 0.16670494 0.94501637 0.15384615 1.         0.08333333]\n [1.         1.         1.         1.         1.         1.        ]\n [0.99975848 0.93922439 0.99998658 0.18181818 1.         0.1       ]\n [0.99994633 0.90244065 0.99994333 0.875      1.         0.77777778]\n [0.99975848 0.70008878 0.99969579 0.         0.         0.        ]\n [0.99991949 0.90932339 0.99995526 0.8        1.         0.66666667]\n [0.99989266 0.63414104 0.85724639 0.6        1.         0.42857143]\n [0.99983899 0.4766081  0.99971242 0.25       1.         0.14285714]\n [0.99997316 1.         1.         0.90909091 1.         0.83333333]\n [0.99991949 0.89309524 0.9999839  0.72727273 0.66666667 0.8       ]\n [1.         1.         1.         1.         1.         1.        ]\n [0.99997316 0.79111111 0.99996243 0.88888889 1.         0.8       ]\n [0.99983899 0.02176357 0.61349473 0.         0.         0.        ]\n [0.99986582 0.91964286 0.9999839  0.         0.         0.        ]]\nSaved all results to /kaggle/working/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL.csv\nSaved each results to /kaggle/working/GNN_DDI/DDI/data5/G_allf_32_cm_each_DDIMDL.csv\n\nTraining final model on all data...\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_1_32.txt: (37264, 32)\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_2_32.txt: (37264, 32)\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_3_32.txt: (37264, 32)\nLoaded features from /kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt: (37264, 32)\nFinal feature matrix shape: (37264, 128)\nFinal DNN model architecture:\n\u001b[1mModel: \"functional_41\"\u001b[0m\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ Inputlayer (\u001b[94mInputLayer\u001b[0m)         │ (\u001b[96mNone\u001b[0m, \u001b[32m128\u001b[0m)            │             \u001b[32m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_60 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │        \u001b[32m66,048\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_40          │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │         \u001b[32m2,048\u001b[0m │\n│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_40 (\u001b[94mDropout\u001b[0m)            │ (\u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m)            │             \u001b[32m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_61 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │       \u001b[32m131,328\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_41          │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │         \u001b[32m1,024\u001b[0m │\n│ (\u001b[94mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_41 (\u001b[94mDropout\u001b[0m)            │ (\u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)            │             \u001b[32m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_62 (\u001b[94mDense\u001b[0m)                │ (\u001b[96mNone\u001b[0m, \u001b[32m65\u001b[0m)             │        \u001b[32m16,705\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ activation_20 (\u001b[94mActivation\u001b[0m)      │ (\u001b[96mNone\u001b[0m, \u001b[32m65\u001b[0m)             │             \u001b[32m0\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n\u001b[1m Total params: \u001b[0m\u001b[32m217,153\u001b[0m (848.25 KB)\n\u001b[1m Trainable params: \u001b[0m\u001b[32m215,617\u001b[0m (842.25 KB)\n\u001b[1m Non-trainable params: \u001b[0m\u001b[32m1,536\u001b[0m (6.00 KB)\nEpoch 1/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.4376 - loss: 2.5637 - val_accuracy: 0.2427 - val_loss: 2.5775\nEpoch 2/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6570 - loss: 1.1827 - val_accuracy: 0.2612 - val_loss: 2.1874\nEpoch 3/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7010 - loss: 0.9649 - val_accuracy: 0.5500 - val_loss: 1.3019\nEpoch 4/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7307 - loss: 0.8534 - val_accuracy: 0.6820 - val_loss: 1.0215\nEpoch 5/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7466 - loss: 0.7966 - val_accuracy: 0.6133 - val_loss: 1.2158\nEpoch 6/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7634 - loss: 0.7246 - val_accuracy: 0.6844 - val_loss: 0.9313\nEpoch 7/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7734 - loss: 0.6941 - val_accuracy: 0.6562 - val_loss: 1.1095\nEpoch 8/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7823 - loss: 0.6678 - val_accuracy: 0.6145 - val_loss: 1.2909\nEpoch 9/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.6534 - val_accuracy: 0.7143 - val_loss: 0.8532\nEpoch 10/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7919 - loss: 0.6198 - val_accuracy: 0.7728 - val_loss: 0.7048\nEpoch 11/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7947 - loss: 0.6132 - val_accuracy: 0.7728 - val_loss: 0.7394\nEpoch 12/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7996 - loss: 0.5954 - val_accuracy: 0.7816 - val_loss: 0.6469\nEpoch 13/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8015 - loss: 0.5852 - val_accuracy: 0.7681 - val_loss: 0.7171\nEpoch 14/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8046 - loss: 0.5795 - val_accuracy: 0.7170 - val_loss: 0.9167\nEpoch 15/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8101 - loss: 0.5572 - val_accuracy: 0.7479 - val_loss: 0.8344\nEpoch 16/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8125 - loss: 0.5585 - val_accuracy: 0.7474 - val_loss: 0.7985\nEpoch 17/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8155 - loss: 0.5451 - val_accuracy: 0.7853 - val_loss: 0.6739\nEpoch 18/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8174 - loss: 0.5349 - val_accuracy: 0.7797 - val_loss: 0.6758\nEpoch 19/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8195 - loss: 0.5307 - val_accuracy: 0.7842 - val_loss: 0.6369\nEpoch 20/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8201 - loss: 0.5359 - val_accuracy: 0.7950 - val_loss: 0.6370\nEpoch 21/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8298 - loss: 0.5060 - val_accuracy: 0.7329 - val_loss: 0.8062\nEpoch 22/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8239 - loss: 0.5070 - val_accuracy: 0.7947 - val_loss: 0.6193\nEpoch 23/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8277 - loss: 0.5089 - val_accuracy: 0.7409 - val_loss: 0.8887\nEpoch 24/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.5027 - val_accuracy: 0.7584 - val_loss: 0.7164\nEpoch 25/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8314 - loss: 0.4953 - val_accuracy: 0.8124 - val_loss: 0.5671\nEpoch 26/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8304 - loss: 0.4944 - val_accuracy: 0.7844 - val_loss: 0.6545\nEpoch 27/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8317 - loss: 0.4887 - val_accuracy: 0.7757 - val_loss: 0.6972\nEpoch 28/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8288 - loss: 0.4915 - val_accuracy: 0.7549 - val_loss: 0.7559\nEpoch 29/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8378 - loss: 0.4669 - val_accuracy: 0.8061 - val_loss: 0.5870\nEpoch 30/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8361 - loss: 0.4814 - val_accuracy: 0.8218 - val_loss: 0.5185\nEpoch 31/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8360 - loss: 0.4740 - val_accuracy: 0.8293 - val_loss: 0.5330\nEpoch 32/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8371 - loss: 0.4680 - val_accuracy: 0.8209 - val_loss: 0.5558\nEpoch 33/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8354 - loss: 0.4727 - val_accuracy: 0.8093 - val_loss: 0.6192\nEpoch 34/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8377 - loss: 0.4747 - val_accuracy: 0.7618 - val_loss: 0.7272\nEpoch 35/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8387 - loss: 0.4714 - val_accuracy: 0.8211 - val_loss: 0.5586\nEpoch 36/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8384 - loss: 0.4581 - val_accuracy: 0.8281 - val_loss: 0.5416\nEpoch 37/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8470 - loss: 0.4487 - val_accuracy: 0.7722 - val_loss: 0.7642\nEpoch 38/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8455 - loss: 0.4536 - val_accuracy: 0.8261 - val_loss: 0.5548\nEpoch 39/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.4588 - val_accuracy: 0.8167 - val_loss: 0.5617\nEpoch 40/100\n\u001b[1m233/233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8475 - loss: 0.4450 - val_accuracy: 0.8189 - val_loss: 0.5477\nEpoch 40: early stopping\nFinal model saved to /kaggle/working/GNN_DDI/DDI/model_trained_final.h5\n\nTime used: 836.56 seconds\n","output_type":"stream"}],"execution_count":9},{"id":"548ae942-3511-429c-a9cb-909d44309cc3","cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# Load the trained model\nmodel = load_model('/kaggle/working/GNN_DDI/DDI/model_trained_final.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:14:01.841323Z","iopub.execute_input":"2024-12-03T20:14:01.841692Z","iopub.status.idle":"2024-12-03T20:14:01.897046Z","shell.execute_reply.started":"2024-12-03T20:14:01.841655Z","shell.execute_reply":"2024-12-03T20:14:01.896361Z"}},"outputs":[],"execution_count":16},{"id":"2bd2bc87-b187-4e7c-bde0-f20156a1fe1c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import load_model\n\n# Load test data\ntest_data = pd.read_csv('/kaggle/working/GNN_DDI/DDI/data5/t_c_m_4_32.txt', sep=' ', header=None)\nprint(\"Raw test data shape:\", test_data.shape)\n\n# Extract features (assume first column is ID or label, exclude it)\nX_test_raw = test_data.iloc[:, 1:].values\n\n# Ensure it matches the model's input shape (128 features)\nif X_test_raw.shape[1] < 128:\n    X_test = np.pad(X_test_raw, ((0, 0), (0, 128 - X_test_raw.shape[1])), mode='constant')\nelif X_test_raw.shape[1] > 128:\n    X_test = X_test_raw[:, :128]\nelse:\n    X_test = X_test_raw\n\nprint(\"Adjusted test data shape:\", X_test.shape)\n\n# Load the trained model\nmodel = load_model('/kaggle/working/GNN_DDI/DDI/model_trained_final.h5')\n\n# Test the model\npredictions = model.predict(X_test)\nprint(\"Shape of predictions:\", predictions.shape)\n\n# Output predictions (e.g., for inspection or further processing)\nprint(\"Predictions:\", predictions.flatten())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:11:43.787558Z","iopub.execute_input":"2024-12-03T21:11:43.788405Z","iopub.status.idle":"2024-12-03T21:11:46.091537Z","shell.execute_reply.started":"2024-12-03T21:11:43.788368Z","shell.execute_reply":"2024-12-03T21:11:46.090595Z"}},"outputs":[{"name":"stdout","text":"Raw test data shape: (37264, 32)\nAdjusted test data shape: (37264, 128)\n\u001b[1m1165/1165\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\nShape of predictions: (37264, 65)\nPredictions: [8.4443384e-36 2.6414294e-02 1.7094469e-10 ... 1.4792247e-40 3.4103808e-29\n 8.4321287e-26]\n","output_type":"stream"}],"execution_count":37}]}